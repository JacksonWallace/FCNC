{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ideal-adjustment",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "equipped-dylan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\n",
    "from coffea import processor, hist\n",
    "\n",
    "from processor.FCNC_cutflow import FCNC_cutflow\n",
    "from Tools.config_helpers import loadConfig\n",
    "from klepto.archives import dir_archive\n",
    "\n",
    "lumi = {2016: 35.9, 2017: 41.5, 2018: 59.71}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cloudy-ownership",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36fe6e768d104acc8843c2ab690a54de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/1096 [00:00<?, ?chunk/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm running now\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f373bba398c40e08c7eec41e7fb74ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/3043 [00:00<?, ?chunk/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from processor.default_accumulators import desired_output, add_processes_to_output\n",
    "\n",
    "from Tools.helpers import get_samples, cutflow_scale_and_merge\n",
    "from Tools.config_helpers import redirector_ucsd, redirector_fnal\n",
    "from Tools.nano_mapping import make_fileset, nano_mapping\n",
    "\n",
    "from processor.meta_processor import get_sample_meta\n",
    "\n",
    "overwrite = True\n",
    "local = True\n",
    "\n",
    "# load the config and the cache\n",
    "cfg = loadConfig()\n",
    "\n",
    "cacheName = 'dielectron_mass'\n",
    "cache = dir_archive(os.path.join(os.path.expandvars(cfg['caches']['base']), cacheName), serialized=True)\n",
    "\n",
    "year = 2018\n",
    "\n",
    "# get a python dictionary of all NanoAOD samples\n",
    "# The samples definitions can be found in data/samples.yaml\n",
    "samples = get_samples(year)\n",
    "\n",
    "# make a fileset, taking the definitions in Tools/nano_mapping.py\n",
    "nano_mappings = nano_mapping(year)\n",
    "fileset = make_fileset(['Fakes_Flips', 'Rares', 'hct', 'hut'], year, redirector=redirector_ucsd, small=False)\n",
    "meta = get_sample_meta(fileset, samples)\n",
    "# 'DY', 'Rares', 'TT/TW', 'TTH', 'TTW', 'TTZ', 'TTVV', 'W', 'X+g'\n",
    "# in order for cutflows to work we need to add every process to the output accumulator\n",
    "add_processes_to_output(fileset, desired_output)\n",
    "\n",
    "histograms = sorted(list(desired_output.keys()))\n",
    "\n",
    "if local:\n",
    "\n",
    "    exe_args = {\n",
    "        'workers': 16,\n",
    "        'function_args': {'flatten': False},\n",
    "        \"schema\": NanoAODSchema,\n",
    "        \"skipbadfiles\": True,\n",
    "    }\n",
    "    exe = processor.futures_executor\n",
    "\n",
    "else:\n",
    "    from Tools.helpers import get_scheduler_address\n",
    "    from dask.distributed import Client, progress\n",
    "\n",
    "    scheduler_address = get_scheduler_address()\n",
    "    c = Client(scheduler_address)\n",
    "\n",
    "    exe_args = {\n",
    "        'client': c,\n",
    "        'function_args': {'flatten': False},\n",
    "        \"schema\": NanoAODSchema,\n",
    "        \"skipbadfiles\": True,\n",
    "    }\n",
    "    exe = processor.dask_executor\n",
    "\n",
    "\n",
    "if not overwrite:\n",
    "    cache.load()\n",
    "\n",
    "if cfg == cache.get('cfg') and histograms == cache.get('histograms') and cache.get('simple_output'):\n",
    "    output = cache.get('simple_output')\n",
    "    \n",
    "else:\n",
    "    print (\"I'm running now\")\n",
    "\n",
    "    output = processor.run_uproot_job(\n",
    "        fileset,\n",
    "        \"Events\",\n",
    "        FCNC_cutflow(year=year, variations=[], accumulator=desired_output),\n",
    "        exe,\n",
    "        exe_args,\n",
    "        chunksize=250000,\n",
    "    )\n",
    "\n",
    "    cache['fileset']        = fileset\n",
    "    cache['cfg']            = cfg\n",
    "    cache['histograms']     = histograms\n",
    "    cache['simple_output']  = output\n",
    "    cache.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-julian",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['totalEvents']['all']/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12a904bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if year == 2016:\n",
    "    output['hct'] = output['/TT_FCNC-TtoHJ_aTleptonic_HToWWZZtautau_eta_hct-MadGraph5-pythia8/RunIISummer16NanoAODv7-PUMoriond17_Nano02Apr2020_102X_mcRun2_asymptotic_v8-v1/NANOAODSIM'] + output['/TT_FCNC-aTtoHJ_Tleptonic_HToWWZZtautau_eta_hct-MadGraph5-pythia8/RunIISummer16NanoAODv7-PUMoriond17_Nano02Apr2020_102X_mcRun2_asymptotic_v8-v1/NANOAODSIM']\n",
    "    output['hut'] = output['/TT_FCNC-TtoHJ_aTleptonic_HToWWZZtautau_eta_hut-MadGraph5-pythia8/RunIISummer16NanoAODv7-PUMoriond17_Nano02Apr2020_102X_mcRun2_asymptotic_v8-v1/NANOAODSIM'] + output['/TT_FCNC-aTtoHJ_Tleptonic_HToWWZZtautau_eta_hut-MadGraph5-pythia8/RunIISummer16NanoAODv7-PUMoriond17_Nano02Apr2020_102X_mcRun2_asymptotic_v8-v1/NANOAODSIM']\n",
    "\n",
    "if year == 2017:\n",
    "    output['hct'] = output['/TT_FCNC-TtoHJ_aTleptonic_HToWWZZtautau_eta_hct_TuneCP5-MadGraph5-pythia8/RunIIFall17NanoAODv7-PU2017_12Apr2018_Nano02Apr2020_tauDecays_102X_mc2017_realistic_v8-v1/NANOAODSIM'] + output['/TT_FCNC-aTtoHJ_Tleptonic_HToWWZZtautau_eta_hct_TuneCP5-MadGraph5-pythia8/RunIIFall17NanoAODv7-PU2017_12Apr2018_Nano02Apr2020_tauDecays_102X_mc2017_realistic_v8-v1/NANOAODSIM']\n",
    "    output['hut'] = output['/TT_FCNC-TtoHJ_aTleptonic_HToWWZZtautau_eta_hut_TuneCP5-MadGraph5-pythia8/RunIIFall17NanoAODv7-PU2017_12Apr2018_Nano02Apr2020_tauDecays_102X_mc2017_realistic_v8-v1/NANOAODSIM'] + output['/TT_FCNC-aTtoHJ_Tleptonic_HToWWZZtautau_eta_hut_TuneCP5-MadGraph5-pythia8/RunIIFall17NanoAODv7-PU2017_12Apr2018_Nano02Apr2020_tauDecays_102X_mc2017_realistic_v8-v1/NANOAODSIM'] \n",
    "\n",
    "if year == 2018:\n",
    "    output['hct'] = output['/TT_FCNC-TtoHJ_aTleptonic_HToWWZZtautau_eta_hct_TuneCP5-MadGraph5-pythia8/RunIIAutumn18NanoAODv7-Nano02Apr2020_tauDecays_102X_upgrade2018_realistic_v21-v1/NANOAODSIM'] + output['/TT_FCNC-aTtoHJ_Tleptonic_HToWWZZtautau_eta_hct_TuneCP5-MadGraph5-pythia8/RunIIAutumn18NanoAODv7-Nano02Apr2020_tauDecays_102X_upgrade2018_realistic_v21-v1/NANOAODSIM'] #+ output['/ST_FCNC-TH_Tleptonic_HToWWZZtautau_eta_hct-MadGraph5-pythia8/RunIIAutumn18NanoAODv7-Nano02Apr2020_tauDecays_102X_upgrade2018_realistic_v21-v1/NANOAODSIM']\n",
    "    output['hut'] = output['/TT_FCNC-TtoHJ_aTleptonic_HToWWZZtautau_eta_hut_TuneCP5-MadGraph5-pythia8/RunIIAutumn18NanoAODv7-Nano02Apr2020_tauDecays_102X_upgrade2018_realistic_v21-v1/NANOAODSIM'] + output['/TT_FCNC-aTtoHJ_Tleptonic_HToWWZZtautau_eta_hut_TuneCP5-MadGraph5-pythia8/RunIIAutumn18NanoAODv7-Nano02Apr2020_tauDecays_102X_upgrade2018_realistic_v21-v1/NANOAODSIM'] #+ output['/ST_FCNC-TH_Tleptonic_HToWWZZtautau_eta_hut-MadGraph5-pythia8/RunIIAutumn18NanoAODv7-Nano02Apr2020_tauDecays_102X_upgrade2018_realistic_v21-v1/NANOAODSIM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c953265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hct</th>\n",
       "      <th>hut</th>\n",
       "      <th>S/B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>entry</th>\n",
       "      <td>1956689.0 +/- 1399.0</td>\n",
       "      <td>1924814.0 +/- 1387.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"skim\"</th>\n",
       "      <td>108100.0 +/- 328.8</td>\n",
       "      <td>106502.0 +/- 326.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lepton selection</th>\n",
       "      <td>77597.0 +/- 278.56</td>\n",
       "      <td>76513.0 +/- 276.61</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leading lepton</th>\n",
       "      <td>76950.0 +/- 277.4</td>\n",
       "      <td>75897.0 +/- 275.49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>triggers</th>\n",
       "      <td>71187.0 +/- 266.81</td>\n",
       "      <td>70102.0 +/- 264.77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filter</th>\n",
       "      <td>71152.0 +/- 266.74</td>\n",
       "      <td>70069.0 +/- 264.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ss</th>\n",
       "      <td>61181.0 +/- 247.35</td>\n",
       "      <td>60097.0 +/- 245.15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SS onZ veto</th>\n",
       "      <td>58655.0 +/- 242.19</td>\n",
       "      <td>57615.0 +/- 240.03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two jets</th>\n",
       "      <td>47472.0 +/- 217.88</td>\n",
       "      <td>47449.0 +/- 217.83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MET &gt; 50</th>\n",
       "      <td>31621.0 +/- 177.82</td>\n",
       "      <td>31834.0 +/- 178.42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   hct                   hut  S/B\n",
       "entry             1956689.0 +/- 1399.0  1924814.0 +/- 1387.0    1\n",
       "\"skim\"              108100.0 +/- 328.8    106502.0 +/- 326.3    1\n",
       "lepton selection    77597.0 +/- 278.56    76513.0 +/- 276.61    1\n",
       "leading lepton       76950.0 +/- 277.4    75897.0 +/- 275.49    1\n",
       "triggers            71187.0 +/- 266.81    70102.0 +/- 264.77    1\n",
       "filter              71152.0 +/- 266.74    70069.0 +/- 264.71    1\n",
       "ss                  61181.0 +/- 247.35    60097.0 +/- 245.15    1\n",
       "SS onZ veto         58655.0 +/- 242.19    57615.0 +/- 240.03    1\n",
       "two jets            47472.0 +/- 217.88    47449.0 +/- 217.83    1\n",
       "MET > 50            31621.0 +/- 177.82    31834.0 +/- 178.42    1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Tools.helpers import getCutFlowTable, cutflow_scale_and_merge\n",
    "\n",
    "lines = ['entry', '\"skim\"', 'lepton selection', 'leading lepton', 'triggers', 'filter', 'ss', 'SS onZ veto', 'two jets', 'MET > 50']\n",
    "#lines2 = ['entry', '\"skim\"', 'lepton selection', 'leading lepton', 'triggers', 'filter', 'multilep', 'one jet', 'MET > 50']\n",
    "\n",
    "output2 = cutflow_scale_and_merge(output, meta, fileset, nano_mappings, lumi=lumi[year])\n",
    "df = getCutFlowTable(output, processes=['hct', 'hut'], absolute=True, lines=lines, significantFigures=7, signal=['hct', 'hut'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f3697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import cloudpickle\n",
    "outname = 'backgrounds_and_signal_ttHiD_'+str(year)\n",
    "os.system(\"mkdir -p histos/\")\n",
    "print('Saving output in %s...'%(\"histos/\" + outname + \".pkl.gz\"))\n",
    "with gzip.open(\"histos/\" + outname + \".pkl.gz\", \"wb\") as fout:\n",
    "    cloudpickle.dump(output, fout)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "excessive-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the plotting libararies: matplotlib and mplhep\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "plt.style.use(hep.style.CMS)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# load the functions to make a nice plot from the output histograms\n",
    "# and the scale_and_merge function that scales the individual histograms\n",
    "# to match the physical cross section\n",
    "\n",
    "from plots.helpers import scale_and_merge\n",
    "\n",
    "# define a few axes that we can use to rebin our output histograms\n",
    "\n",
    "N_bins         = hist.Bin('multiplicity', r'$N$', 10, -0.5, 9.5)\n",
    "N_bins_red     = hist.Bin('multiplicity', r'$N$', 2, -0.5, 0.5)\n",
    "pt_bins        = hist.Bin('pt', r'$p_{T}\\ (GeV)$', np.array([15, 40, 60, 80, 100, 200, 300]))\n",
    "pt_fine_bins   = hist.Bin('pt', r'$p_{T}\\ (GeV)$', 300, 0, 300)\n",
    "pt_rebin       = hist.Bin('pt', r'$p_{T}\\ (GeV)$', 100, 0, 500)\n",
    "pt_rebin2      = hist.Bin('pt', r'$p_{T}\\ (GeV)$', 60, 0, 300)\n",
    "pt_rebin22     = hist.Bin('pt', r'$p_{T}\\ (GeV)$', 100, 0, 3000)\n",
    "eta_bins       = hist.Bin('eta', r'$\\eta $', np.array([0, 0.8, 1.479, 2.5]))\n",
    "eta_rebin      = hist.Bin('eta', r'$\\eta $', 25, -2.5, 2.5) \n",
    "eta_rebin2      = hist.Bin('eta', r'$\\eta $', 50, -5, 5)    \n",
    "phi_bins       = hist.Bin('phi', r'$\\phi $', 16, -3.2, 3.2)\n",
    "mass_bins      = hist.Bin('mass', r'$mass (GeV/c^2)$', 60, 0, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-acrobat",
   "metadata": {},
   "source": [
    "# 1D Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "welcome-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yahist import Hist1D, Hist2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "regulated-sunset",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total(histos, keys):\n",
    "        tmp = Hist1D.from_bincounts(np.zeros(len(histos[keys[0]].counts)), histos[keys[0]].edges, )\n",
    "        for key in keys:\n",
    "            tmp += histos[key]\n",
    "        return tmp\n",
    "\n",
    "def add_uncertainty(hist, ax, ratio=False):\n",
    "    opts = {'step': 'post', 'label': 'Uncertainty', 'hatch': '///',\n",
    "                    'facecolor': 'none', 'edgecolor': (0, 0, 0, .5), 'linewidth': 0, 'zorder':10.}\n",
    "    \n",
    "    if ratio:\n",
    "        down = np.ones(len(hist.counts)) - hist.errors/hist.counts\n",
    "        up = np.ones(len(hist.counts)) + hist.errors/hist.counts\n",
    "    else:\n",
    "        down = hist.counts-hist.errors\n",
    "        up = hist.counts+hist.errors\n",
    "    ax.fill_between(x=hist.edges, y1=np.r_[down, down[-1]], y2=np.r_[up, up[-1]], **opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e993fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(c, ce):\n",
    "    out = \"{:2g} \\n $\\pm${:.2f}\".format(c, ce)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8fdc36",
   "metadata": {},
   "source": [
    "# Limits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8135f399",
   "metadata": {},
   "source": [
    "## Yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3948c137",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = scale_and_merge(output['j_vs_b_ss'], meta, fileset, nano_mappings, lumi=lumi[year])\n",
    "tmp2 = scale_and_merge(output['j_vs_b_ss_flips'], meta, fileset, nano_mappings, lumi=lumi[year])\n",
    "tmp3 = scale_and_merge(output['j_vs_b_ss_fakes'], meta, fileset, nano_mappings, lumi=lumi[year])\n",
    "tmp4 = scale_and_merge(output['j_vs_b_ss_non_fakes_flips'], meta, fileset, nano_mappings, lumi=lumi[year])\n",
    "\n",
    "\n",
    "h1_hut = Hist2D.from_bincounts(\n",
    "    tmp1.values(overflow = 'over')[('hut',)].T,\n",
    "    (tmp1.axis('n1').edges(overflow = 'over'), tmp1.axis('n2').edges(overflow = 'over')),\n",
    "    errors = np.sqrt(tmp1.values(sumw2=True, overflow = 'over')[('hut',)][1].T),\n",
    ")\n",
    "\n",
    "h1_hut = h1_hut*0.01\n",
    "\n",
    "h1_hct = Hist2D.from_bincounts(\n",
    "    tmp1.values(overflow = 'over')[('hct',)].T,\n",
    "    (tmp1.axis('n1').edges(overflow = 'over'), tmp1.axis('n2').edges(overflow = 'over')),\n",
    "    errors = np.sqrt(tmp1.values(sumw2=True, overflow = 'over')[('hct',)][1].T),\n",
    ")\n",
    "\n",
    "h1_hct = h1_hct*0.01\n",
    "\n",
    "h1_rare = Hist2D.from_bincounts(\n",
    "    tmp1.values(overflow = 'over')[('Rares',)].T,\n",
    "    (tmp1.axis('n1').edges(overflow = 'over'), tmp1.axis('n2').edges(overflow = 'over')),\n",
    "    errors = np.sqrt(tmp1.values(sumw2=True, overflow = 'over')[('Rares',)][1].T),\n",
    ")\n",
    "\n",
    "h1_flip = Hist2D.from_bincounts(\n",
    "    tmp2.values(overflow = 'over')[('Fakes_Flips',)].T,\n",
    "    (tmp2.axis('n1').edges(overflow = 'over'), tmp2.axis('n2').edges(overflow = 'over')),\n",
    "    errors = np.sqrt(tmp2.values(sumw2=True, overflow = 'over')[('Fakes_Flips',)][1].T),\n",
    ")\n",
    "\n",
    "h1_fake = Hist2D.from_bincounts(\n",
    "    tmp3.values(overflow = 'over')[('Fakes_Flips',)].T,\n",
    "    (tmp3.axis('n1').edges(overflow = 'over'), tmp3.axis('n2').edges(overflow = 'over')),\n",
    "    errors = np.sqrt(tmp3.values(sumw2=True, overflow = 'over')[('Fakes_Flips',)][1].T),\n",
    ")\n",
    "\n",
    "h1_non_fake_flip = Hist2D.from_bincounts(\n",
    "    tmp4.values(overflow = 'over')[('Fakes_Flips',)].T,\n",
    "    (tmp4.axis('n1').edges(overflow = 'over'), tmp4.axis('n2').edges(overflow = 'over')),\n",
    "    errors = np.sqrt(tmp4.values(sumw2=True, overflow = 'over')[('Fakes_Flips',)][1].T),\n",
    ")\n",
    "\n",
    "h1_non_fake_flip_r = Hist2D.from_bincounts(\n",
    "    tmp4.values(overflow = 'over')[('Rares',)].T,\n",
    "    (tmp4.axis('n1').edges(overflow = 'over'), tmp4.axis('n2').edges(overflow = 'over')),\n",
    "    errors = np.sqrt(tmp4.values(sumw2=True, overflow = 'over')[('Rares',)][1].T),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baae6ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h1_hct.plot(show_counts=True, equidistant='xy', counts_formatter=f)\n",
    "ax.set_xlabel(r'$N_{jets}$')\n",
    "ax.set_ylabel(r'$N_{b}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1883e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h1_hut.plot(show_counts=True, equidistant='xy', counts_formatter=f)\n",
    "ax.set_xlabel(r'$N_{jets}$')\n",
    "ax.set_ylabel(r'$N_{b}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1dc967",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h1_fake.plot(show_counts=True, equidistant='xy', counts_formatter=f)\n",
    "ax.set_xlabel(r'$N_{jets}$')\n",
    "ax.set_ylabel(r'$N_{b}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "99397188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1201.9634041422898"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(h1_fake.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2515fffd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h1_flip.plot(show_counts=True, equidistant='xy', counts_formatter=f)\n",
    "ax.set_xlabel(r'$N_{jets}$')\n",
    "ax.set_ylabel(r'$N_{b}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "779c3fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.72630812654178"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(h1_flip.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd3f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h1_rare.plot(show_counts=True, equidistant='xy', counts_formatter=f)\n",
    "ax.set_xlabel(r'$N_{jets}$')\n",
    "ax.set_ylabel(r'$N_{b}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5d693f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1524.6661248688392"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(h1_rare.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba89df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h1_non_fake_flip.plot(show_counts=True, equidistant='xy', counts_formatter=f)\n",
    "ax.set_xlabel(r'$N_{jets}$')\n",
    "ax.set_ylabel(r'$N_{b}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "933c005e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161.0425518008649"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(h1_non_fake_flip.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decb9221",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp5 = scale_and_merge(output['j_vs_b_ml'], meta, fileset, nano_mappings, lumi=lumi[year])\n",
    "\n",
    "\n",
    "h2_hut = Hist2D.from_bincounts(\n",
    "    tmp5.values(overflow = 'all')[('hut',)].T,\n",
    "    (tmp5.axis('n1').edges(overflow = 'all'), tmp5.axis('n2').edges(overflow = 'all')),\n",
    "    errors = np.sqrt(tmp5.values(sumw2=True, overflow = 'all')[('hut',)][1].T),\n",
    ")\n",
    "\n",
    "h2_hut = h2_hut*0.01\n",
    "\n",
    "h2_hct = Hist2D.from_bincounts(\n",
    "    tmp5.values(overflow = 'all')[('hct',)] .T,\n",
    "    (tmp5.axis('n1').edges(overflow = 'all'), tmp5.axis('n2').edges(overflow = 'all')),\n",
    "    errors = np.sqrt(tmp5.values(sumw2=True, overflow = 'all')[('hct',)][1].T),\n",
    ")\n",
    "\n",
    "h2_hct = h2_hct*0.01\n",
    "\n",
    "h2_fake = Hist2D.from_bincounts(\n",
    "    tmp2.values(overflow = 'all')[('Flips',)].T,\n",
    "    (tmp2.axis('n1').edges(overflow = 'all'), tmp2.axis('n2').edges(overflow = 'all')),\n",
    "    errors = np.sqrt(tmp2.values(sumw2=True, overflow = 'all')[('Flips',)][1].T),\n",
    ")\n",
    "\n",
    "h2_flip_bins = [np.array([0.5, 1.5, 2.5, 3.5, 4.5]),\n",
    "                np.array([-1.5, -0.5, 0.5, 1.5, 2.5]),]\n",
    "            \n",
    "h2_flip_counts = np.array([[0.01, 0.01, 0.01, 0.01],\n",
    "                           [0.01, 0.01, 0.01, 0.01],\n",
    "                           [0.01, 0.01, 0.01, 0.01],\n",
    "                           [0.01, 0.01, 0.01, 0.01],])\n",
    "\n",
    "h2_flip_errors = np.array([[0.01, 0.01, 0.01, 0.01],\n",
    "                           [0.01, 0.01, 0.01, 0.01],\n",
    "                           [0.01, 0.01, 0.01, 0.01],\n",
    "                           [0.01, 0.01, 0.01, 0.01],])\n",
    "            \n",
    "h2_flip = Hist2D.from_bincounts(h2_flip_counts, h2_flip_bins, h2_flip_errors)\n",
    "\n",
    "h2_rare = Hist2D.from_bincounts(\n",
    "    tmp2.values(overflow = 'all')[('Rares',)].T,\n",
    "    (tmp2.axis('n1').edges(overflow = 'all'), tmp2.axis('n2').edges(overflow = 'all')),\n",
    "    errors = np.sqrt(tmp2.values(sumw2=True, overflow = 'all')[('Rares',)][1].T),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h2_hct.plot(counts=True, equidistant='xy', counts_formatter=f)\n",
    "ax.set_xlabel(r'$N_{jets}$')\n",
    "ax.set_ylabel(r'$N_{b}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0280098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h2_hut.plot(counts=True, equidistant='xy', counts_formatter=f)\n",
    "ax.set_xlabel(r'$N_{jets}$')\n",
    "ax.set_ylabel(r'$N_{b}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dfa4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h2_fake.plot(counts=True, equidistant='xy', counts_formatter=f)\n",
    "ax.set_xlabel(r'$N_{jets}$')\n",
    "ax.set_ylabel(r'$N_{b}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0a4191",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h2_flip.plot(counts=True, equidistant='xy', counts_formatter=f)\n",
    "ax.set_xlabel(r'$N_{jets}$')\n",
    "ax.set_ylabel(r'$N_{b}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17995526",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h2_rare.plot(counts=True, equidistant='xy', counts_formatter=f)\n",
    "ax.set_xlabel(r'$N_{jets}$')\n",
    "ax.set_ylabel(r'$N_{b}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f90a45",
   "metadata": {},
   "source": [
    "## Datacards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bb8398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tools.dataCard import dataCard\n",
    "\n",
    "c = dataCard()\n",
    "c.setPrecision(3)\n",
    "\n",
    "c.addUncertainty('fakerate',    'lnN')\n",
    "c.addUncertainty('fliprate',    'lnN')\n",
    "c.addUncertainty('rare_norm',   'lnN')\n",
    "c.addUncertainty('signal_norm', 'lnN')\n",
    "c.addUncertainty('lumi',        'lnN')\n",
    "\n",
    "binnum = 0\n",
    "\n",
    "\n",
    "for b in range(0,3):\n",
    "    for j in range(0,3):\n",
    "        binname = 'bin'+str(binnum)\n",
    "        Binname = 'DL_'+str(b)+'_'+str(j+2)\n",
    "        binnum += 1\n",
    "        c.addBin(binname, ['fake', 'flip', 'rare'], Binname) # signal is automatically added\n",
    "        \n",
    "        processes = {'signal': h1_hct, 'fake': h1_fake, 'flip': h1_flip, 'rare': h1_rare}\n",
    "        for process in processes:\n",
    "            uname = 'Stat_'+binname+'_'+process\n",
    "            c.addUncertainty(uname, 'lnN')\n",
    "            c.specifyUncertainty(uname, binname, process, round(1+processes[process].errors[b][j]/processes[process].counts[b][j], 3))\n",
    "        \n",
    "        c.specifyExpectation(binname, 'signal',  round(h1_hct.counts[b][j], 3))\n",
    "        c.specifyExpectation(binname, 'fake',    round(h1_fake.counts[b][j], 3))\n",
    "        c.specifyExpectation(binname, 'flip',    round(h1_flip.counts[b][j], 3))\n",
    "        c.specifyExpectation(binname, 'rare',    round(h1_rare.counts[b][j], 3))\n",
    "\n",
    "        c.specifyUncertainty('signal_norm', binname, 'signal', 1.01)\n",
    "        c.specifyUncertainty('fakerate',    binname, 'fake',   1.40)\n",
    "        c.specifyUncertainty('fliprate',    binname, 'flip',   1.30)\n",
    "        c.specifyUncertainty('rare_norm',   binname, 'rare',   1.30)\n",
    "\n",
    "        c.specifyObservation(binname, round(h1_fake.counts[b][j] + h1_flip.counts[b][j] + h1_rare.counts[b][j], 3))\n",
    "        \n",
    "for b in range(1,4):\n",
    "    for j in range(0,4):\n",
    "        binname = 'bin'+str(binnum)\n",
    "        Binname = 'ML_'+str(b)+'_'+str(j+1)\n",
    "        binnum += 1\n",
    "        \n",
    "        c.addBin(binname, ['fake', 'flip', 'rare'], Binname) # signal is automatically added\n",
    "        \n",
    "        processes = {'signal': h2_hct, 'fake': h2_fake, 'flip': h2_flip, 'rare': h2_rare}\n",
    "        for process in processes:\n",
    "            uname = 'Stat_'+binname+'_'+process\n",
    "            c.addUncertainty(uname, 'lnN') \n",
    "            c.specifyUncertainty(uname, binname, process, round(1+processes[process].errors[b][j]/processes[process].counts[b][j], 3))\n",
    "        \n",
    "        c.specifyExpectation(binname, 'signal',  round(h2_hct.counts[b][j], 3))\n",
    "        c.specifyExpectation(binname, 'fake',    round(h2_fake.counts[b][j], 3))\n",
    "        c.specifyExpectation(binname, 'flip',    round(h2_flip.counts[b][j], 3))\n",
    "        c.specifyExpectation(binname, 'rare',    round(h2_rare.counts[b][j], 3))\n",
    "\n",
    "        c.specifyUncertainty('signal_norm', binname, 'signal', 1.01)\n",
    "        c.specifyUncertainty('fakerate',    binname, 'fake',   1.40)\n",
    "        c.specifyUncertainty('fliprate',    binname, 'flip',   1.30)\n",
    "        c.specifyUncertainty('rare_norm',   binname, 'rare',   1.30)\n",
    "\n",
    "        c.specifyObservation(binname, round(h2_fake.counts[b][j] + h2_flip.counts[b][j] + h2_rare.counts[b][j], 3))\n",
    "    \n",
    "c.specifyFlatUncertainty('lumi', 1.02)\n",
    "\n",
    "\n",
    "c.writeToFile('./FCNC_hct_ttH_'+str(year)+'_1.txt')\n",
    "\n",
    "\n",
    "#res = c.calcLimit('./FCNC_hct_ttH_'+str(year)+'_1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cfc9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tools.dataCard import dataCard\n",
    "\n",
    "c = dataCard()\n",
    "c.setPrecision(3)\n",
    "\n",
    "c.addUncertainty('fakerate', 'lnN')\n",
    "c.addUncertainty('fliprate', 'lnN')\n",
    "c.addUncertainty('rare_norm', 'lnN')\n",
    "c.addUncertainty('signal_norm', 'lnN')\n",
    "c.addUncertainty('lumi', 'lnN')\n",
    "\n",
    "binnum = 0\n",
    "\n",
    "for b in range(0,3):\n",
    "    for j in range(0,3):\n",
    "        binname = 'bin'+str(binnum)\n",
    "        Binname = 'DL_'+str(b)+'_'+str(j+2)\n",
    "        binnum += 1\n",
    "        c.addBin(binname, ['fake', 'flip', 'rare'], Binname) # signal is automatically added\n",
    "        \n",
    "        processes = {'signal': h1_hut, 'fake': h1_fake, 'flip': h1_flip, 'rare': h1_rare}\n",
    "        for process in processes:\n",
    "            uname = 'Stat_'+binname+'_'+process\n",
    "            c.addUncertainty(uname, 'lnN')\n",
    "            c.specifyUncertainty(uname, binname, process, 1+processes[process].errors[b][j]/processes[process].counts[b][j])\n",
    "        \n",
    "        c.specifyExpectation(binname, 'signal',  h1_hut.counts[b][j])\n",
    "        c.specifyExpectation(binname, 'fake',    h1_fake.counts[b][j])\n",
    "        c.specifyExpectation(binname, 'flip',    h1_flip.counts[b][j])\n",
    "        c.specifyExpectation(binname, 'rare',    h1_rare.counts[b][j])\n",
    "\n",
    "        c.specifyUncertainty('signal_norm', binname, 'signal', 1.01)\n",
    "        c.specifyUncertainty('fakerate',    binname, 'fake',   1.40)\n",
    "        c.specifyUncertainty('fliprate',    binname, 'flip',   1.30)\n",
    "        c.specifyUncertainty('rare_norm',   binname, 'rare',   1.30)\n",
    "\n",
    "        c.specifyObservation(binname, h1_fake.counts[b][j]+h1_flip.counts[b][j]+h1_rare.counts[b][j])\n",
    "        \n",
    "for b in range(1,4):\n",
    "    for j in range(0,4):\n",
    "        binname = 'bin'+str(binnum)\n",
    "        Binname = 'ML_'+str(b)+'_'+str(j+1)\n",
    "        binnum += 1\n",
    "        \n",
    "        c.addBin(binname, ['fake', 'flip', 'rare'], Binname) # signal is automatically added\n",
    "        \n",
    "        processes = {'signal': h2_hut, 'fake': h2_fake, 'flip': h2_flip, 'rare': h2_rare}\n",
    "        for process in processes:\n",
    "            uname = 'Stat_'+binname+'_'+process\n",
    "            c.addUncertainty(uname, 'lnN')\n",
    "            c.specifyUncertainty(uname, binname, process, round(1+processes[process].errors[b][j]/processes[process].counts[b][j], 3))\n",
    "        \n",
    "        c.specifyExpectation(binname, 'signal',  h2_hut.counts[b][j])\n",
    "        c.specifyExpectation(binname, 'fake',    h2_fake.counts[b][j])\n",
    "        c.specifyExpectation(binname, 'flip',    h2_flip.counts[b][j])\n",
    "        c.specifyExpectation(binname, 'rare',    h2_rare.counts[b][j])\n",
    "\n",
    "        c.specifyUncertainty('signal_norm', binname, 'signal', 1.01)\n",
    "        c.specifyUncertainty('fakerate',    binname, 'fake',   1.40)\n",
    "        c.specifyUncertainty('fliprate',    binname, 'flip',   1.30)\n",
    "        c.specifyUncertainty('rare_norm',   binname, 'rare',   1.30)\n",
    "\n",
    "        c.specifyObservation(binname, h2_fake.counts[b][j]+h2_flip.counts[b][j]+h2_rare.counts[b][j])\n",
    "    \n",
    "c.specifyFlatUncertainty('lumi', 1.02)\n",
    "\n",
    "\n",
    "c.writeToFile('./FCNC_hut_ttH_'+str(year)+'_1.txt')\n",
    "\n",
    "\n",
    "#res = c.calcLimit('./FCNC_hut_ttH_'+str(year)+'_1.txt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffeadev",
   "language": "python",
   "name": "coffeadev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
