{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-adjustment",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-dylan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\n",
    "from coffea import processor, hist\n",
    "\n",
    "from processor.FCNC_cutflow import *\n",
    "from Tools.config_helpers import loadConfig\n",
    "from klepto.archives import dir_archive\n",
    "\n",
    "lumi = {2016: 35.9, 2017: 41.5, 2018: 59.71}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-ownership",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from processor.default_accumulators import desired_output, add_processes_to_output\n",
    "\n",
    "from Tools.helpers import get_samples, cutflow_scale_and_merge\n",
    "from Tools.config_helpers import redirector_ucsd, redirector_fnal\n",
    "from Tools.nano_mapping import make_fileset, nano_mapping\n",
    "\n",
    "from processor.meta_processor import get_sample_meta\n",
    "\n",
    "overwrite = True\n",
    "local = True\n",
    "small = False\n",
    "dump = True\n",
    "\n",
    "# load the config and the cache\n",
    "cfg = loadConfig()\n",
    "\n",
    "cacheName = 'dielectron_mass'\n",
    "cache = dir_archive(os.path.join(os.path.expandvars(cfg['caches']['base']), cacheName), serialized=True)\n",
    "\n",
    "# get a python dictionary of all NanoAOD samples\n",
    "# The samples definitions can be found in data/samples.yaml\n",
    "\n",
    "# in order for cutflows to work we need to add every process to the output accumulator\n",
    "add_processes_to_output(fileset, desired_output)\n",
    "\n",
    "if dump:\n",
    "    variables = [\n",
    "        'lead_lep_pt',\n",
    "        'sublead_lep_pt',\n",
    "        'subsublead_lep_pt',\n",
    "        'lead_lep_eta',\n",
    "        'sublead_lep_eta',\n",
    "        'subsublead_lep_eta',\n",
    "        'lead_lep_dxy',\n",
    "        'sublead_lep_dxy',\n",
    "        'subsublead_lep_dxy',\n",
    "        'lead_lep_dz',\n",
    "        'sublead_lep_dz',\n",
    "        'subsublead_lep_dz',\n",
    "        'lead_lep_MET_MT',\n",
    "        'sublead_lep_MET_MT',\n",
    "        'subsublead_lep_MET_MT',\n",
    "        'lead_jet_pt',\n",
    "        'sublead_jet_pt',\n",
    "        'subsublead_jet_pt',\n",
    "        'lead_jet_btag_score',\n",
    "        'sublead_jet_btag_score',\n",
    "        'subsublead_jet_btag_score',\n",
    "        'MET_pt',\n",
    "        'forward_jet_pt',\n",
    "        'HT',\n",
    "        'n_electrons',\n",
    "        'n_jets',\n",
    "        'n_btags',\n",
    "        'lead_btag_pt',\n",
    "        'lead_btag_btag_score',\n",
    "        'sub_lead_lead_mass',\n",
    "        'label',\n",
    "        'weight',\n",
    "    ]\n",
    "    \n",
    "    for var in variables:\n",
    "        desired_output.update({var: processor.column_accumulator(np.zeros(shape=(0,)))})\n",
    "        \n",
    "\n",
    "histograms = sorted(list(desired_output.keys()))\n",
    "\n",
    "if local:\n",
    "\n",
    "    exe_args = {\n",
    "        'workers': 16,\n",
    "        'function_args': {'flatten': False},\n",
    "        \"schema\": NanoAODSchema,\n",
    "        \"skipbadfiles\": True,\n",
    "    }\n",
    "    exe = processor.futures_executor\n",
    "\n",
    "else:\n",
    "    from Tools.helpers import get_scheduler_address\n",
    "    from dask.distributed import Client, progress\n",
    "\n",
    "    scheduler_address = get_scheduler_address()\n",
    "    c = Client(scheduler_address)\n",
    "\n",
    "    exe_args = {\n",
    "        'client': c,\n",
    "        'function_args': {'flatten': False},\n",
    "        \"schema\": NanoAODSchema,\n",
    "        \"skipbadfiles\": True,\n",
    "    }\n",
    "    exe = processor.dask_executor\n",
    "\n",
    "\n",
    "if not overwrite:\n",
    "    cache.load()\n",
    "\n",
    "if cfg == cache.get('cfg') and histograms == cache.get('histograms') and cache.get('simple_output'):\n",
    "    output = cache.get('simple_output')\n",
    "    \n",
    "else:\n",
    "    print (\"I'm running now\")\n",
    "\n",
    "    output = processor.run_uproot_job(\n",
    "        fileset,\n",
    "        \"Events\",\n",
    "        FCNC_cutflow(year=year, variations=[], accumulator=desired_output, dump=dump),\n",
    "        exe,\n",
    "        exe_args,\n",
    "        chunksize=250000,\n",
    "    )\n",
    "\n",
    "    cache['fileset']        = fileset\n",
    "    cache['cfg']            = cfg\n",
    "    cache['histograms']     = histograms\n",
    "    cache['simple_output']  = output\n",
    "    cache.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84540f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dump:\n",
    "    if overwrite:\n",
    "        df_dict = {} \n",
    "        for var in variables:\n",
    "            #tmp_output = scale_and_merge(output[var], meta, fileset, nano_mappings, lumi=lumi[year])\n",
    "            df_dict.update({var: output[var].value})  \n",
    "\n",
    "        df_out = pd.DataFrame( df_dict )\n",
    "        if not small:\n",
    "            df_out.to_hdf('FCNC_BDT_input_%s_v2.h5'%year, key='df', format='table', mode='w')\n",
    "    else:\n",
    "        print (\"Loading DF\")\n",
    "        df_out = pd.read_hdf('FCNC_BDT_input_%s_v2.h5'%year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5d6796",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f012003",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out[df_out['label']>=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-julian",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['totalEvents']['all']/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a904bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if year == 2016:\n",
    "    output['hct'] = output['/TT_FCNC-TtoHJ_aTleptonic_HToWWZZtautau_eta_hct-MadGraph5-pythia8/RunIISummer16NanoAODv7-PUMoriond17_Nano02Apr2020_102X_mcRun2_asymptotic_v8-v1/NANOAODSIM'] + output['/TT_FCNC-aTtoHJ_Tleptonic_HToWWZZtautau_eta_hct-MadGraph5-pythia8/RunIISummer16NanoAODv7-PUMoriond17_Nano02Apr2020_102X_mcRun2_asymptotic_v8-v1/NANOAODSIM']\n",
    "    output['hut'] = output['/TT_FCNC-TtoHJ_aTleptonic_HToWWZZtautau_eta_hut-MadGraph5-pythia8/RunIISummer16NanoAODv7-PUMoriond17_Nano02Apr2020_102X_mcRun2_asymptotic_v8-v1/NANOAODSIM'] + output['/TT_FCNC-aTtoHJ_Tleptonic_HToWWZZtautau_eta_hut-MadGraph5-pythia8/RunIISummer16NanoAODv7-PUMoriond17_Nano02Apr2020_102X_mcRun2_asymptotic_v8-v1/NANOAODSIM']\n",
    "\n",
    "if year == 2017:\n",
    "    output['hct'] = output['/TT_FCNC-TtoHJ_aTleptonic_HToWWZZtautau_eta_hct_TuneCP5-MadGraph5-pythia8/RunIIFall17NanoAODv7-PU2017_12Apr2018_Nano02Apr2020_tauDecays_102X_mc2017_realistic_v8-v1/NANOAODSIM'] + output['/TT_FCNC-aTtoHJ_Tleptonic_HToWWZZtautau_eta_hct_TuneCP5-MadGraph5-pythia8/RunIIFall17NanoAODv7-PU2017_12Apr2018_Nano02Apr2020_tauDecays_102X_mc2017_realistic_v8-v1/NANOAODSIM']\n",
    "    output['hut'] = output['/TT_FCNC-TtoHJ_aTleptonic_HToWWZZtautau_eta_hut_TuneCP5-MadGraph5-pythia8/RunIIFall17NanoAODv7-PU2017_12Apr2018_Nano02Apr2020_tauDecays_102X_mc2017_realistic_v8-v1/NANOAODSIM'] + output['/TT_FCNC-aTtoHJ_Tleptonic_HToWWZZtautau_eta_hut_TuneCP5-MadGraph5-pythia8/RunIIFall17NanoAODv7-PU2017_12Apr2018_Nano02Apr2020_tauDecays_102X_mc2017_realistic_v8-v1/NANOAODSIM'] \n",
    "\n",
    "if year == 2018:\n",
    "    output['hct'] = output['/TT_FCNC-TtoHJ_aTleptonic_HToWWZZtautau_eta_hct_TuneCP5-MadGraph5-pythia8/RunIIAutumn18NanoAODv7-Nano02Apr2020_tauDecays_102X_upgrade2018_realistic_v21-v1/NANOAODSIM'] + output['/TT_FCNC-aTtoHJ_Tleptonic_HToWWZZtautau_eta_hct_TuneCP5-MadGraph5-pythia8/RunIIAutumn18NanoAODv7-Nano02Apr2020_tauDecays_102X_upgrade2018_realistic_v21-v1/NANOAODSIM'] #+ output['/ST_FCNC-TH_Tleptonic_HToWWZZtautau_eta_hct-MadGraph5-pythia8/RunIIAutumn18NanoAODv7-Nano02Apr2020_tauDecays_102X_upgrade2018_realistic_v21-v1/NANOAODSIM']\n",
    "    output['hut'] = output['/TT_FCNC-TtoHJ_aTleptonic_HToWWZZtautau_eta_hut_TuneCP5-MadGraph5-pythia8/RunIIAutumn18NanoAODv7-Nano02Apr2020_tauDecays_102X_upgrade2018_realistic_v21-v1/NANOAODSIM'] + output['/TT_FCNC-aTtoHJ_Tleptonic_HToWWZZtautau_eta_hut_TuneCP5-MadGraph5-pythia8/RunIIAutumn18NanoAODv7-Nano02Apr2020_tauDecays_102X_upgrade2018_realistic_v21-v1/NANOAODSIM'] #+ output['/ST_FCNC-TH_Tleptonic_HToWWZZtautau_eta_hut-MadGraph5-pythia8/RunIIAutumn18NanoAODv7-Nano02Apr2020_tauDecays_102X_upgrade2018_realistic_v21-v1/NANOAODSIM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c953265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tools.helpers import getCutFlowTable, cutflow_scale_and_merge\n",
    "\n",
    "lines = ['entry', '\"skim\"', 'lepton selection', 'leading lepton', 'triggers', 'filter', 'ss', 'SS onZ veto', 'two jets', 'MET > 50']\n",
    "#lines2 = ['entry', '\"skim\"', 'lepton selection', 'leading lepton', 'triggers', 'filter', 'multilep', 'one jet', 'MET > 50']\n",
    "\n",
    "output2 = cutflow_scale_and_merge(output, meta, fileset, nano_mappings, lumi=lumi[year])\n",
    "df = getCutFlowTable(output2, processes=['Fakes_Flips', 'Rares', 'hct', 'hut'], absolute=True, lines=lines, significantFigures=7, signal=['hct', 'hut'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f3697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import cloudpickle\n",
    "outname = 'ttjets1l'+str(year)\n",
    "os.system(\"mkdir -p histos/\")\n",
    "print('Saving output in %s...'%(\"histos/\" + outname + \".pkl.gz\"))\n",
    "with gzip.open(\"histos/\" + outname + \".pkl.gz\", \"wb\") as fout:\n",
    "    cloudpickle.dump(output, fout)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the plotting libararies: matplotlib and mplhep\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "plt.style.use(hep.style.CMS)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# load the functions to make a nice plot from the output histograms\n",
    "# and the scale_and_merge function that scales the individual histograms\n",
    "# to match the physical cross section\n",
    "\n",
    "from plots.helpers import scale_and_merge\n",
    "\n",
    "# define a few axes that we can use to rebin our output histograms\n",
    "\n",
    "N_bins         = hist.Bin('multiplicity', r'$N$', 10, -0.5, 9.5)\n",
    "N_bins_red     = hist.Bin('multiplicity', r'$N$', 2, -0.5, 0.5)\n",
    "pt_bins        = hist.Bin('pt', r'$p_{T}\\ (GeV)$', np.array([15, 40, 60, 80, 100, 200, 300]))\n",
    "pt_fine_bins   = hist.Bin('pt', r'$p_{T}\\ (GeV)$', 300, 0, 300)\n",
    "pt_rebin       = hist.Bin('pt', r'$p_{T}\\ (GeV)$', 100, 0, 500)\n",
    "pt_rebin2      = hist.Bin('pt', r'$p_{T}\\ (GeV)$', 60, 0, 300)\n",
    "pt_rebin22     = hist.Bin('pt', r'$p_{T}\\ (GeV)$', 100, 0, 3000)\n",
    "eta_bins       = hist.Bin('eta', r'$\\eta $', np.array([0, 0.8, 1.479, 2.5]))\n",
    "eta_rebin      = hist.Bin('eta', r'$\\eta $', 25, -2.5, 2.5) \n",
    "eta_rebin2      = hist.Bin('eta', r'$\\eta $', 50, -5, 5)    \n",
    "phi_bins       = hist.Bin('phi', r'$\\phi $', 16, -3.2, 3.2)\n",
    "mass_bins      = hist.Bin('mass', r'$mass (GeV/c^2)$', 60, 0, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-acrobat",
   "metadata": {},
   "source": [
    "# 1D Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yahist import Hist1D, Hist2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-sunset",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total(histos, keys):\n",
    "        tmp = Hist1D.from_bincounts(np.zeros(len(histos[keys[0]].counts)), histos[keys[0]].edges, )\n",
    "        for key in keys:\n",
    "            tmp += histos[key]\n",
    "        return tmp\n",
    "\n",
    "def add_uncertainty(hist, ax, ratio=False):\n",
    "    opts = {'step': 'post', 'label': 'Uncertainty', 'hatch': '///',\n",
    "                    'facecolor': 'none', 'edgecolor': (0, 0, 0, .5), 'linewidth': 0, 'zorder':10.}\n",
    "    \n",
    "    if ratio:\n",
    "        down = np.ones(len(hist.counts)) - hist.errors/hist.counts\n",
    "        up = np.ones(len(hist.counts)) + hist.errors/hist.counts\n",
    "    else:\n",
    "        down = hist.counts-hist.errors\n",
    "        up = hist.counts+hist.errors\n",
    "    ax.fill_between(x=hist.edges, y1=np.r_[down, down[-1]], y2=np.r_[up, up[-1]], **opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e993fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(c, ce):\n",
    "    out = \"{:2g} \\n $\\pm${:.2f}\".format(c, ce)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8fdc36",
   "metadata": {},
   "source": [
    "# Limits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8135f399",
   "metadata": {},
   "source": [
    "## Yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3948c137",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = scale_and_merge(output['j_vs_b_ss'], meta, fileset, nano_mappings, lumi=lumi[year])\n",
    "tmp2 = scale_and_merge(output['j_vs_b_ss_flips'], meta, fileset, nano_mappings, lumi=lumi[year])\n",
    "tmp3 = scale_and_merge(output['j_vs_b_ss_fakes'], meta, fileset, nano_mappings, lumi=lumi[year])\n",
    "tmp4 = scale_and_merge(output['j_vs_b_ss_non_fakes_flips'], meta, fileset, nano_mappings, lumi=lumi[year])\n",
    "\n",
    "\n",
    "h1_hut = Hist2D.from_bincounts(\n",
    "    tmp1.values(overflow = 'over')[('hut',)].T,\n",
    "    (tmp1.axis('n1').edges(overflow = 'over'), tmp1.axis('n2').edges(overflow = 'over')),\n",
    "    errors = np.sqrt(tmp1.values(sumw2=True, overflow = 'over')[('hut',)][1].T),\n",
    ")\n",
    "\n",
    "h1_hct = Hist2D.from_bincounts(\n",
    "    tmp1.values(overflow = 'over')[('hct',)].T,\n",
    "    (tmp1.axis('n1').edges(overflow = 'over'), tmp1.axis('n2').edges(overflow = 'over')),\n",
    "    errors = np.sqrt(tmp1.values(sumw2=True, overflow = 'over')[('hct',)][1].T),\n",
    ")\n",
    "\n",
    "h1_rare = Hist2D.from_bincounts(\n",
    "    tmp1.values(overflow = 'over')[('Rares',)].T,\n",
    "    (tmp1.axis('n1').edges(overflow = 'over'), tmp1.axis('n2').edges(overflow = 'over')),\n",
    "    errors = np.sqrt(tmp1.values(sumw2=True, overflow = 'over')[('Rares',)][1].T),\n",
    ")\n",
    "\n",
    "h1_flip = Hist2D.from_bincounts(\n",
    "    tmp2.values(overflow = 'over')[('Fakes_Flips',)].T,\n",
    "    (tmp2.axis('n1').edges(overflow = 'over'), tmp2.axis('n2').edges(overflow = 'over')),\n",
    "    errors = np.sqrt(tmp2.values(sumw2=True, overflow = 'over')[('Fakes_Flips',)][1].T),\n",
    ")\n",
    "\n",
    "h1_fake = Hist2D.from_bincounts(\n",
    "    tmp3.values(overflow = 'over')[('Fakes_Flips',)].T,\n",
    "    (tmp3.axis('n1').edges(overflow = 'over'), tmp3.axis('n2').edges(overflow = 'over')),\n",
    "    errors = np.sqrt(tmp3.values(sumw2=True, overflow = 'over')[('Fakes_Flips',)][1].T),\n",
    ")\n",
    "\n",
    "h1_fake_flip_rare = Hist2D.from_bincounts(\n",
    "    tmp4.values(overflow = 'over')[('Fakes_Flips',)].T,\n",
    "    (tmp4.axis('n1').edges(overflow = 'over'), tmp4.axis('n2').edges(overflow = 'over')),\n",
    "    errors = np.sqrt(tmp4.values(sumw2=True, overflow = 'over')[('Fakes_Flips',)][1].T),\n",
    ")\n",
    "\n",
    "h1_rare_fake = Hist2D.from_bincounts(\n",
    "    tmp3.values(overflow = 'over')[('Rares',)].T,\n",
    "    (tmp3.axis('n1').edges(overflow = 'over'), tmp3.axis('n2').edges(overflow = 'over')),\n",
    "    errors = np.sqrt(tmp3.values(sumw2=True, overflow = 'over')[('Rares',)][1].T),\n",
    ")\n",
    "\n",
    "h1_rare_flip = Hist2D.from_bincounts(\n",
    "    tmp2.values(overflow = 'over')[('Rares',)].T,\n",
    "    (tmp2.axis('n1').edges(overflow = 'over'), tmp2.axis('n2').edges(overflow = 'over')),\n",
    "    errors = np.sqrt(tmp2.values(sumw2=True, overflow = 'over')[('Rares',)][1].T),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baae6ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h1_hct.plot(show_counts=True, equidistant='xy', counts_formatter=f)\n",
    "ax.set_xlabel(r'$N_{jets}$')\n",
    "ax.set_ylabel(r'$N_{b}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1883e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h1_hut.plot(show_counts=True, equidistant='xy', counts_formatter=f)\n",
    "ax.set_xlabel(r'$N_{jets}$')\n",
    "ax.set_ylabel(r'$N_{b}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1dc967",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h1_fake.plot(show_counts=True, equidistant='xy', counts_formatter=f)\n",
    "ax.set_xlabel(r'$N_{jets}$')\n",
    "ax.set_ylabel(r'$N_{b}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99397188",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(h1_fake.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2515fffd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h1_flip.plot(show_counts=True, equidistant='xy', counts_formatter=f)\n",
    "ax.set_xlabel(r'$N_{jets}$')\n",
    "ax.set_ylabel(r'$N_{b}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779c3fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(h1_flip.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd3f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h1_rare.plot(show_counts=True, equidistant='xy', counts_formatter=f)\n",
    "ax.set_xlabel(r'$N_{jets}$')\n",
    "ax.set_ylabel(r'$N_{b}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d693f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(h1_rare.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba89df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h1_fake_flip_rare.plot(show_counts=True, equidistant='xy', counts_formatter=f)\n",
    "ax.set_xlabel(r'$N_{jets}$')\n",
    "ax.set_ylabel(r'$N_{b}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933c005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(h1_fake_flip_rare.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed11b6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h1_rare_fake.plot(show_counts=True, equidistant='xy', counts_formatter=f)\n",
    "ax.set_xlabel(r'$N_{jets}$')\n",
    "ax.set_ylabel(r'$N_{b}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c240c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.sum(h1_rare_fake.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29576c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h1_rare_flip.plot(show_counts=True, equidistant='xy', counts_formatter=f)\n",
    "ax.set_xlabel(r'$N_{jets}$')\n",
    "ax.set_ylabel(r'$N_{b}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bf1fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(h1_rare_flip.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decb9221",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp5 = scale_and_merge(output['j_vs_b_ml'], meta, fileset, nano_mappings, lumi=lumi[year])\n",
    "tmp6 = scale_and_merge(output['j_vs_b_ml_fakes'], meta, fileset, nano_mappings, lumi=lumi[year])\n",
    "tmp7 = scale_and_merge(output['j_vs_b_ml_non_fakes'], meta, fileset, nano_mappings, lumi=lumi[year])\n",
    "\n",
    "\n",
    "h2_hut = Hist2D.from_bincounts(\n",
    "    tmp5.values(overflow = 'all')[('hut',)].T,\n",
    "    (tmp5.axis('n1').edges(overflow = 'all'), tmp5.axis('n2').edges(overflow = 'all')),\n",
    "    errors = np.sqrt(tmp5.values(sumw2=True, overflow = 'all')[('hut',)][1].T),\n",
    ")\n",
    "\n",
    "h2_hct = Hist2D.from_bincounts(\n",
    "    tmp5.values(overflow = 'all')[('hct',)] .T,\n",
    "    (tmp5.axis('n1').edges(overflow = 'all'), tmp5.axis('n2').edges(overflow = 'all')),\n",
    "    errors = np.sqrt(tmp5.values(sumw2=True, overflow = 'all')[('hct',)][1].T),\n",
    ")\n",
    "\n",
    "h2_fake = Hist2D.from_bincounts(\n",
    "    tmp6.values(overflow = 'all')[('Fakes_Flips',)].T,\n",
    "    (tmp6.axis('n1').edges(overflow = 'all'), tmp6.axis('n2').edges(overflow = 'all')),\n",
    "    errors = np.sqrt(tmp6.values(sumw2=True, overflow = 'all')[('Fakes_Flips',)][1].T),\n",
    ")\n",
    "\n",
    "h2_flip_bins = [np.array([0.5, 1.5, 2.5, 3.5, 4.5]),\n",
    "                np.array([-1.5, -0.5, 0.5, 1.5, 2.5]),]\n",
    "            \n",
    "h2_flip_counts = np.array([[0.01, 0.01, 0.01, 0.01],\n",
    "                           [0.01, 0.01, 0.01, 0.01],\n",
    "                           [0.01, 0.01, 0.01, 0.01],\n",
    "                           [0.01, 0.01, 0.01, 0.01],])\n",
    "\n",
    "h2_flip_errors = np.array([[0.01, 0.01, 0.01, 0.01],\n",
    "                           [0.01, 0.01, 0.01, 0.01],\n",
    "                           [0.01, 0.01, 0.01, 0.01],\n",
    "                           [0.01, 0.01, 0.01, 0.01],])\n",
    "            \n",
    "h2_flip = Hist2D.from_bincounts(h2_flip_counts, h2_flip_bins, h2_flip_errors)\n",
    "\n",
    "h2_rare = Hist2D.from_bincounts(\n",
    "    tmp5.values(overflow = 'all')[('Rares',)].T,\n",
    "    (tmp5.axis('n1').edges(overflow = 'all'), tmp5.axis('n2').edges(overflow = 'all')),\n",
    "    errors = np.sqrt(tmp5.values(sumw2=True, overflow = 'all')[('Rares',)][1].T),\n",
    ")\n",
    "\n",
    "h2_fake_rare = Hist2D.from_bincounts(\n",
    "    tmp7.values(overflow = 'all')[('Fakes_Flips',)].T,\n",
    "    (tmp7.axis('n1').edges(overflow = 'all'), tmp7.axis('n2').edges(overflow = 'all')),\n",
    "    errors = np.sqrt(tmp7.values(sumw2=True, overflow = 'all')[('Fakes_Flips',)][1].T),\n",
    ")\n",
    "\n",
    "h2_rare_fake = Hist2D.from_bincounts(\n",
    "    tmp6.values(overflow = 'all')[('Rares',)].T,\n",
    "    (tmp6.axis('n1').edges(overflow = 'all'), tmp6.axis('n2').edges(overflow = 'all')),\n",
    "    errors = np.sqrt(tmp6.values(sumw2=True, overflow = 'all')[('Rares',)][1].T),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h2_hct.plot(counts=True, equidistant='xy', counts_formatter=f)\n",
    "ax.set_xlabel(r'$N_{jets}$')\n",
    "ax.set_ylabel(r'$N_{b}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0280098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h2_hut.plot(counts=True, equidistant='xy', counts_formatter=f)\n",
    "ax.set_xlabel(r'$N_{jets}$')\n",
    "ax.set_ylabel(r'$N_{b}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dfa4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h2_fake.plot(counts=True, equidistant='xy', counts_formatter=f)\n",
    "ax.set_xlabel(r'$N_{jets}$')\n",
    "ax.set_ylabel(r'$N_{b}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0a4191",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h2_flip.plot(counts=True, equidistant='xy', counts_formatter=f)\n",
    "ax.set_xlabel(r'$N_{jets}$')\n",
    "ax.set_ylabel(r'$N_{b}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17995526",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h2_rare.plot(counts=True, equidistant='xy', counts_formatter=f)\n",
    "ax.set_xlabel(r'$N_{jets}$')\n",
    "ax.set_ylabel(r'$N_{b}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b9c6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h2_fake_rare.plot(counts=True, equidistant='xy', counts_formatter=f)\n",
    "ax.set_xlabel(r'$N_{jets}$')\n",
    "ax.set_ylabel(r'$N_{b}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b50d002",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h2_rare_fake.plot(counts=True, equidistant='xy', counts_formatter=f)\n",
    "ax.set_xlabel(r'$N_{jets}$')\n",
    "ax.set_ylabel(r'$N_{b}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f90a45",
   "metadata": {},
   "source": [
    "## Datacards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bb8398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tools.dataCard import dataCard\n",
    "\n",
    "c = dataCard()\n",
    "c.setPrecision(3)\n",
    "\n",
    "c.addUncertainty('fakerate',    'lnN')\n",
    "c.addUncertainty('fliprate',    'lnN')\n",
    "c.addUncertainty('rare_norm',   'lnN')\n",
    "c.addUncertainty('signal_norm', 'lnN')\n",
    "c.addUncertainty('lumi',        'lnN')\n",
    "\n",
    "binnum = 0\n",
    "\n",
    "\n",
    "for b in range(0,3):\n",
    "    for j in range(0,3):\n",
    "        binname = 'bin'+str(binnum)\n",
    "        Binname = 'DL_'+str(b+1)+'_'+str(j+2)\n",
    "        binnum += 1\n",
    "        c.addBin(binname, ['fake', 'flip', 'rare'], Binname) # signal is automatically added\n",
    "        \n",
    "        processes = {'signal': h1_hct, 'fake': h1_fake, 'flip': h1_flip, 'rare': h1_rare}\n",
    "        for process in processes:\n",
    "            uname = 'Stat_'+binname+'_'+process\n",
    "            c.addUncertainty(uname, 'lnN')\n",
    "            c.specifyUncertainty(uname, binname, process, round(1+processes[process].errors[b][j]/processes[process].counts[b][j], 3))\n",
    "        \n",
    "        c.specifyExpectation(binname, 'signal',  round(h1_hct.counts[b][j], 3))\n",
    "        c.specifyExpectation(binname, 'fake',    round(h1_fake.counts[b][j], 3))\n",
    "        c.specifyExpectation(binname, 'flip',    round(h1_flip.counts[b][j], 3))\n",
    "        c.specifyExpectation(binname, 'rare',    round(h1_rare.counts[b][j], 3))\n",
    "\n",
    "        c.specifyUncertainty('signal_norm', binname, 'signal', 1.01)\n",
    "        c.specifyUncertainty('fakerate',    binname, 'fake',   1.40)\n",
    "        c.specifyUncertainty('fliprate',    binname, 'flip',   1.30)\n",
    "        c.specifyUncertainty('rare_norm',   binname, 'rare',   1.30)\n",
    "\n",
    "        c.specifyObservation(binname, round(h1_fake.counts[b][j] + h1_flip.counts[b][j] + h1_rare.counts[b][j], 3))\n",
    "        \n",
    "for b in range(1,4):\n",
    "    for j in range(0,4):\n",
    "        binname = 'bin'+str(binnum)\n",
    "        Binname = 'ML_'+str(b)+'_'+str(j+1)\n",
    "        binnum += 1\n",
    "        \n",
    "        c.addBin(binname, ['fake', 'flip', 'rare'], Binname) # signal is automatically added\n",
    "        \n",
    "        processes = {'signal': h2_hct, 'fake': h2_fake, 'flip': h2_flip, 'rare': h2_rare}\n",
    "        for process in processes:\n",
    "            uname = 'Stat_'+binname+'_'+process\n",
    "            c.addUncertainty(uname, 'lnN') \n",
    "            c.specifyUncertainty(uname, binname, process, round(1+processes[process].errors[b][j]/processes[process].counts[b][j], 3))\n",
    "        \n",
    "        c.specifyExpectation(binname, 'signal',  round(h2_hct.counts[b][j], 3))\n",
    "        c.specifyExpectation(binname, 'fake',    round(h2_fake.counts[b][j], 3))\n",
    "        c.specifyExpectation(binname, 'flip',    round(h2_flip.counts[b][j], 3))\n",
    "        c.specifyExpectation(binname, 'rare',    round(h2_rare.counts[b][j], 3))\n",
    "\n",
    "        c.specifyUncertainty('signal_norm', binname, 'signal', 1.01)\n",
    "        c.specifyUncertainty('fakerate',    binname, 'fake',   1.40)\n",
    "        c.specifyUncertainty('fliprate',    binname, 'flip',   1.30)\n",
    "        c.specifyUncertainty('rare_norm',   binname, 'rare',   1.30)\n",
    "\n",
    "        c.specifyObservation(binname, round(h2_fake.counts[b][j] + h2_flip.counts[b][j] + h2_rare.counts[b][j], 3))\n",
    "    \n",
    "c.specifyFlatUncertainty('lumi', 1.02)\n",
    "\n",
    "\n",
    "c.writeToFile('./FCNC_hct_ttH_'+str(year)+'_2.txt')\n",
    "\n",
    "\n",
    "#res = c.calcLimit('./FCNC_hct_ttH_'+str(year)+'_1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cfc9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tools.dataCard import dataCard\n",
    "\n",
    "c = dataCard()\n",
    "c.setPrecision(3)\n",
    "\n",
    "c.addUncertainty('fakerate', 'lnN')\n",
    "c.addUncertainty('fliprate', 'lnN')\n",
    "c.addUncertainty('rare_norm', 'lnN')\n",
    "c.addUncertainty('signal_norm', 'lnN')\n",
    "c.addUncertainty('lumi', 'lnN')\n",
    "\n",
    "binnum = 0\n",
    "\n",
    "for b in range(0,3):\n",
    "    for j in range(0,3):\n",
    "        binname = 'bin'+str(binnum)\n",
    "        Binname = 'DL_'+str(b)+'_'+str(j+2)\n",
    "        binnum += 1\n",
    "        c.addBin(binname, ['fake', 'flip', 'rare'], Binname) # signal is automatically added\n",
    "        \n",
    "        processes = {'signal': h1_hut, 'fake': h1_fake, 'flip': h1_flip, 'rare': h1_rare}\n",
    "        for process in processes:\n",
    "            uname = 'Stat_'+binname+'_'+process\n",
    "            c.addUncertainty(uname, 'lnN')\n",
    "            c.specifyUncertainty(uname, binname, process, 1+processes[process].errors[b][j]/processes[process].counts[b][j])\n",
    "        \n",
    "        c.specifyExpectation(binname, 'signal',  h1_hut.counts[b][j])\n",
    "        c.specifyExpectation(binname, 'fake',    h1_fake.counts[b][j])\n",
    "        c.specifyExpectation(binname, 'flip',    h1_flip.counts[b][j])\n",
    "        c.specifyExpectation(binname, 'rare',    h1_rare.counts[b][j])\n",
    "\n",
    "        c.specifyUncertainty('signal_norm', binname, 'signal', 1.01)\n",
    "        c.specifyUncertainty('fakerate',    binname, 'fake',   1.40)\n",
    "        c.specifyUncertainty('fliprate',    binname, 'flip',   1.30)\n",
    "        c.specifyUncertainty('rare_norm',   binname, 'rare',   1.30)\n",
    "\n",
    "        c.specifyObservation(binname, round(h1_fake.counts[b][j]+h1_flip.counts[b][j]+h1_rare.counts[b][j], 3))\n",
    "        \n",
    "for b in range(1,4):\n",
    "    for j in range(0,4):\n",
    "        binname = 'bin'+str(binnum)\n",
    "        Binname = 'ML_'+str(b+1)+'_'+str(j+1)\n",
    "        binnum += 1\n",
    "        \n",
    "        c.addBin(binname, ['fake', 'flip', 'rare'], Binname) # signal is automatically added\n",
    "        \n",
    "        processes = {'signal': h2_hut, 'fake': h2_fake, 'flip': h2_flip, 'rare': h2_rare}\n",
    "        for process in processes:\n",
    "            uname = 'Stat_'+binname+'_'+process\n",
    "            c.addUncertainty(uname, 'lnN')\n",
    "            c.specifyUncertainty(uname, binname, process, round(1+processes[process].errors[b][j]/processes[process].counts[b][j], 3))\n",
    "        \n",
    "        c.specifyExpectation(binname, 'signal',  h2_hut.counts[b][j])\n",
    "        c.specifyExpectation(binname, 'fake',    h2_fake.counts[b][j])\n",
    "        c.specifyExpectation(binname, 'flip',    h2_flip.counts[b][j])\n",
    "        c.specifyExpectation(binname, 'rare',    h2_rare.counts[b][j])\n",
    "\n",
    "        c.specifyUncertainty('signal_norm', binname, 'signal', 1.01)\n",
    "        c.specifyUncertainty('fakerate',    binname, 'fake',   1.40)\n",
    "        c.specifyUncertainty('fliprate',    binname, 'flip',   1.30)\n",
    "        c.specifyUncertainty('rare_norm',   binname, 'rare',   1.30)\n",
    "\n",
    "        c.specifyObservation(binname, round(h2_fake.counts[b][j]+h2_flip.counts[b][j]+h2_rare.counts[b][j], 3))\n",
    "    \n",
    "c.specifyFlatUncertainty('lumi', 1.02)\n",
    "\n",
    "\n",
    "c.writeToFile('./FCNC_hut_ttH_'+str(year)+'_2.txt')\n",
    "\n",
    "\n",
    "#res = c.calcLimit('./FCNC_hut_ttH_'+str(year)+'_1.txt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffeadev",
   "language": "python",
   "name": "coffeadev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
