{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example for analysis of NanoAOD samples\n",
    "\n",
    "In this example we don't need any pre-processing of NanoAOD samples and can still use several tools of the tW_scattering repository.\n",
    "\n",
    "- Get the proper normalization for samples\n",
    "- Categorize different samples into process categories\n",
    "- Use coffea processors for the map-reduce step\n",
    "- Make \"nice\" histograms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\n",
    "from coffea import processor, hist\n",
    "\n",
    "from processor.charge_flip_calc import charge_flip_calc\n",
    "from Tools.config_helpers import loadConfig\n",
    "from klepto.archives import dir_archive\n",
    "import time\n",
    "\n",
    "year = 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from processor.default_accumulators import desired_output, add_processes_to_output\n",
    "\n",
    "from Tools.helpers import get_samples\n",
    "from Tools.config_helpers import redirector_ucsd, redirector_ucsd_mini, redirector_fnal\n",
    "from Tools.nano_mapping import make_fileset, nano_mapping\n",
    "\n",
    "from processor.meta_processor import get_sample_meta\n",
    "\n",
    "overwrite = True\n",
    "local = True\n",
    "\n",
    "# load the config and the cache\n",
    "cfg = loadConfig()\n",
    "\n",
    "cacheName = 'charge_flip_calc'\n",
    "cache = dir_archive(os.path.join(os.path.expandvars(cfg['caches']['base']), cacheName), serialized=True)\n",
    "\n",
    "# get a python dictionary of all NanoAOD samples\n",
    "# The samples definitions can be found in data/samples.yaml\n",
    "samples = get_samples(year)\n",
    "\n",
    "# make a fileset, taking the definitions in Tools/nano_mapping.py\n",
    "fileset = make_fileset(['DY', 'top', 'TTZ'], year, redirector=redirector_ucsd_mini, small=False) \n",
    "\n",
    "# in order for cutflows to work we need to add every process to the output accumulator\n",
    "add_processes_to_output(fileset, desired_output)\n",
    "\n",
    "histograms = sorted(list(desired_output.keys()))\n",
    "\n",
    "#meta = get_sample_meta(fileset, samples)\n",
    "\n",
    "chunksize = 250000\n",
    "\n",
    "if local:\n",
    "\n",
    "    exe_args = {\n",
    "        'workers': 16,\n",
    "        'function_args': {'flatten': False},\n",
    "        \"schema\": NanoAODSchema,\n",
    "        \"skipbadfiles\": True,\n",
    "    }\n",
    "    exe = processor.futures_executor\n",
    "\n",
    "else:\n",
    "    from Tools.helpers import get_scheduler_address\n",
    "    from dask.distributed import Client, progress\n",
    "    \n",
    "    scheduler_address = get_scheduler_address()\n",
    "    c = Client(scheduler_address)\n",
    "    \n",
    "    def unique(filename):\n",
    "        file, ext = os.path.splitext(filename)\n",
    "        counter = 0\n",
    "        while os.path.exists(filename):\n",
    "            counter += 1\n",
    "            filename = file + str(counter) + ext\n",
    "        return filename\n",
    "\n",
    "    tstart = time.time()\n",
    "    \n",
    "    from dask.distributed import performance_report\n",
    "    fname = unique(\"dask/dask-report_chunksize=\" + str(chunksize/1000) + \"K.html\")\n",
    "    \n",
    "    exe_args = {\n",
    "        'client': c,\n",
    "        'function_args': {'flatten': False},\n",
    "        \"schema\": NanoAODSchema,\n",
    "        \"skipbadfiles\": True,\n",
    "        'savemetrics': True\n",
    "    }\n",
    "    exe = processor.dask_executor\n",
    "\n",
    "if not overwrite:\n",
    "    cache.load()\n",
    "\n",
    "if cfg == cache.get('cfg') and histograms == cache.get('histograms') and cache.get('simple_output'):\n",
    "    output = cache.get('simple_output')\n",
    "\n",
    "else:\n",
    "    print (\"I'm running now\")\n",
    "    #with performance_report(filename=fname):\n",
    "    output = processor.run_uproot_job(\n",
    "        fileset,\n",
    "        \"Events\",\n",
    "        charge_flip_calc(year=year, variations=[], accumulator=desired_output),\n",
    "        exe,\n",
    "        exe_args,\n",
    "        chunksize=chunksize,\n",
    "        )\n",
    "\n",
    "    cache['fileset']        = fileset\n",
    "    cache['cfg']            = cfg\n",
    "    cache['histograms']     = histograms\n",
    "    cache['simple_output']  = output\n",
    "    cache.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['totalEvents']['all']/1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full fileset is 180M events, and that's basically just DY and ttbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the plotting libararies: matplotlib and mplhep\n",
    "from Tools.nano_mapping import make_fileset, nano_mapping\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "plt.style.use(hep.style.CMS)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# load the functions to make a nice plot from the output histograms\n",
    "# and the scale_and_merge function that scales the individual histograms\n",
    "# to match the physical cross section\n",
    "\n",
    "from plots.helpers import makePlot, scale_and_merge\n",
    "\n",
    "# define a few axes that we can use to rebin our output histograms\n",
    "\n",
    "N_bins         = hist.Bin('multiplicity', r'$N$', 10, -0.5, 9.5)\n",
    "N_bins_red     = hist.Bin('multiplicity', r'$N$', 5, -0.5, 4.5)\n",
    "pt_bins        = hist.Bin('pt', r'$p_{T}\\ (GeV)$', np.array([15, 40, 60, 80, 100, 200, 300]))\n",
    "pt_fine_bins   = hist.Bin('pt', r'$p_{T}\\ (GeV)$', 300, 0, 300)\n",
    "eta_bins       = hist.Bin('eta', r'$\\eta $', np.array([0, 0.8, 1.479, 2.5]))\n",
    "phi_bins       = hist.Bin('phi', r'$\\phi $', 16, -3.2, 3.2)\n",
    "\n",
    "\n",
    "# define nicer labels and colors\n",
    "\n",
    "nano_mappings = nano_mapping(year)\n",
    "\n",
    "my_labels = {\n",
    "    nano_mappings['TTW'][0]: 'ttW',\n",
    "    nano_mappings['TTZ'][0]: 'ttZ',\n",
    "    nano_mappings['DY'][0]: 'DY',\n",
    "    nano_mappings['top'][0]: 't/tt+jets',\n",
    "}\n",
    "\n",
    "my_colors = {\n",
    "    nano_mappings['TTW'][0]: '#8AC926',\n",
    "    nano_mappings['TTZ'][0]: '#FFCA3A',\n",
    "    nano_mappings['DY'][0]: '#6A4C93',\n",
    "    nano_mappings['top'][0]: '#1982C4',\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yahist import Hist1D, Hist2D\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = output['mva_id2'].copy()\n",
    "\n",
    "tmp3 = output['mva_id'].copy()\n",
    "#tmp3 = tmp3.rebin('eta', eta_bins)\n",
    "#tmp3 = tmp3.rebin('pt', pt_bins)\n",
    "\n",
    "\n",
    "tmp4 = output['isolation'].copy()\n",
    "#tmp4 = tmp4.rebin('eta', eta_bins)\n",
    "#tmp4 = tmp4.rebin('pt', pt_bins)\n",
    "\n",
    "h1 = Hist2D.from_bincounts(\n",
    "    tmp1.sum('dataset').values()[()].T,\n",
    "    (tmp1.axis('mva_id').edges(), tmp1.axis('pt').edges()),\n",
    ")\n",
    "\n",
    "h3 = Hist2D.from_bincounts(\n",
    "    tmp3.sum('dataset').values()[()].T,\n",
    "    (tmp3.axis('mva_id').edges(), tmp3.axis('eta').edges()),\n",
    ")\n",
    "\n",
    "\n",
    "h4 = Hist2D.from_bincounts(\n",
    "    tmp4.sum('dataset').values()[()].T,\n",
    "    (tmp4.axis('isolation1').edges(), tmp4.axis('isolation2').edges()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(hep.style.CMS)\n",
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h1.plot(show_counts=True, equidistant='xy')\n",
    "ax.set_xlabel(r'$mva\\ ID$')\n",
    "ax.set_ylabel(r'$p_{T}\\ (GeV)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(hep.style.CMS)\n",
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h3.plot(show_counts=True, equidistant='xy')\n",
    "ax.set_xlabel(r'$mva\\ ID$')\n",
    "ax.set_ylabel(r'$etaSC$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(hep.style.CMS)\n",
    "fig, ax  = plt.subplots(1, 1,figsize=(10,10) )\n",
    "h4.plot(show_counts=True, equidistant='xy')\n",
    "ax.set_xlabel(r'$jetRelIso$')\n",
    "ax.set_ylabel(r'$jetPtRelv2$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daskanalysisenv",
   "language": "python",
   "name": "daskanalysisenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
