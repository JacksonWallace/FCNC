{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of 1511.03674\n",
    "\n",
    "[1511.03674](https://arxiv.org/abs/1511.03674) is the motivation for this work.\n",
    "It contains an example analysis for Run3 (300/fb), that has a rather simple preselection and adds a few cuts on top.\n",
    "They are implemented in the notebook below, and make mostly use of the forward jet.\n",
    "\n",
    "The preselection is discussed as being:\n",
    "- Same sign leptons\n",
    "- at least 4 jets (including forward jets), 3 of them central\n",
    "- at least one b-tagged jets\n",
    "\n",
    "In order to suppress more ttbar, we require at least 5 jets.\n",
    "\n",
    "One of the discussed cuts is to ask for a lepton with >100 GeV.\n",
    "This cut is only ~40% efficient for signal (tW scattering, ttW), and doesn't reject a lot of background.\n",
    "For now, we don't use it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from klepto.archives import dir_archive\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import coffea.processor as processor\n",
    "from coffea.processor.accumulator import AccumulatorABC\n",
    "from coffea.analysis_objects import JaggedCandidateArray\n",
    "from coffea.btag_tools import BTagScaleFactor\n",
    "from coffea import hist\n",
    "import pandas as pd\n",
    "import uproot_methods\n",
    "import uproot\n",
    "import awkward\n",
    "import copy\n",
    "\n",
    "from memory_profiler import profile\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "from Tools.helpers import loadConfig, getCutFlowTable, mergeArray\n",
    "\n",
    "from Tools.objects import Collections\n",
    "from Tools.cutflow import Cutflow\n",
    "\n",
    "# This just tells matplotlib not to open any\n",
    "# interactive windows.\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "def pad_and_flatten(val): \n",
    "    try:\n",
    "        return val.pad(1, clip=True).fillna(0.).flatten()#.reshape(-1, 1)\n",
    "    except AttributeError:\n",
    "        return val.flatten()\n",
    "\n",
    "#model = tf.keras.models.load_model('../ML/data/training.h5')#, custom_objects=None, compile=False)\n",
    "\n",
    "#model._make_predict_function()\n",
    "#graph = tf.get_default_graph()\n",
    "\n",
    "#def run_model(inputs):\n",
    "#    global graph\n",
    "#    with graph.as_default():\n",
    "#        outputs = model.predict(inputs)\n",
    "#    return outputs\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "from keras.models import load_model\n",
    "\n",
    "#model = load_model('../ML/data/training.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "print(sys.getrecursionlimit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class analysisProcessor(processor.ProcessorABC):\n",
    "    \"\"\"Processor used for running the analysis\"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        ## load b-tag SFs\n",
    "        #self.btag_sf = BTagScaleFactor(os.path.expandvars(\"$TWHOME/data/DeepCSV_102XSF_V1.btag.csv.gz\", \"reshape\")\n",
    "\n",
    "        ## load the NN\n",
    "        self.model = load_model('../ML/data/training.h5')\n",
    "        self.stds  = pd.read_json('../ML/data/stds.json').squeeze()\n",
    "        self.means = pd.read_json('../ML/data/means.json').squeeze()\n",
    "        \n",
    "        # we can use a large number of bins and rebin later\n",
    "        dataset_axis        = hist.Cat(\"dataset\",   \"Primary dataset\")\n",
    "        pt_axis             = hist.Bin(\"pt\",        r\"$p_{T}$ (GeV)\", 1000, 0, 1000)\n",
    "        ht_axis             = hist.Bin(\"ht\",        r\"$H_{T}$ (GeV)\", 500, 0, 5000)\n",
    "        mass_axis           = hist.Bin(\"mass\",      r\"M (GeV)\", 1000, 0, 2000)\n",
    "        eta_axis            = hist.Bin(\"eta\",       r\"$\\eta$\", 60, -5.5, 5.5)\n",
    "        multiplicity_axis   = hist.Bin(\"multiplicity\",         r\"N\", 20, -0.5, 19.5)\n",
    "        norm_axis            = hist.Bin(\"norm\",         r\"N\", 25, 0, 1)\n",
    "\n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            \"MET_pt\" :          hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            \"pt_spec_max\" :          hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            \"eta_spec_max\" :          hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "            \"MT\" :          hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            \"HT\" :          hist.Hist(\"Counts\", dataset_axis, ht_axis),\n",
    "            \"ST\" :          hist.Hist(\"Counts\", dataset_axis, ht_axis),\n",
    "            \"mbj_max\" :          hist.Hist(\"Counts\", dataset_axis, mass_axis),\n",
    "            \"mjj_max\" :          hist.Hist(\"Counts\", dataset_axis, mass_axis),\n",
    "            \"mjj_max_baseline\" :  hist.Hist(\"Counts\", dataset_axis, mass_axis),\n",
    "            \"mlb_max\" :          hist.Hist(\"Counts\", dataset_axis, mass_axis),\n",
    "            \"mlb_min\" :          hist.Hist(\"Counts\", dataset_axis, mass_axis),\n",
    "            \"mlj_max\" :          hist.Hist(\"Counts\", dataset_axis, mass_axis),\n",
    "            \"mlj_min\" :          hist.Hist(\"Counts\", dataset_axis, mass_axis),\n",
    "            \"N_b\" :             hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "            \"N_ele\" :             hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "            \"N_diele\" :             hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "            \"N_mu\" :             hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "            \"N_dimu\" :             hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "            \"N_jet\" :           hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "            \"N_spec\" :           hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "            \"FWMT1\" :           hist.Hist(\"Counts\", dataset_axis, norm_axis),\n",
    "            \"FWMT2\" :           hist.Hist(\"Counts\", dataset_axis, norm_axis),\n",
    "            \"FWMT3\" :           hist.Hist(\"Counts\", dataset_axis, norm_axis),\n",
    "            \"FWMT4\" :           hist.Hist(\"Counts\", dataset_axis, norm_axis),\n",
    "            \"FWMT5\" :           hist.Hist(\"Counts\", dataset_axis, norm_axis),\n",
    "            \"S\" :               hist.Hist(\"Counts\", dataset_axis, norm_axis),\n",
    "            \"S_lep\" :           hist.Hist(\"Counts\", dataset_axis, norm_axis),\n",
    "            'diboson':          processor.defaultdict_accumulator(int),\n",
    "            'ttbar':            processor.defaultdict_accumulator(int),\n",
    "            'TTW':              processor.defaultdict_accumulator(int),\n",
    "            'TTZ':              processor.defaultdict_accumulator(int),\n",
    "            'TTH':              processor.defaultdict_accumulator(int),\n",
    "            'TTTT':             processor.defaultdict_accumulator(int),\n",
    "            'tW_scattering':    processor.defaultdict_accumulator(int),\n",
    "            'DY':               processor.defaultdict_accumulator(int),\n",
    "            'totalEvents':      processor.defaultdict_accumulator(int),\n",
    "            'test1':            processor.defaultdict_accumulator(float),\n",
    "        })\n",
    "\n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "\n",
    "    def process(self, df):\n",
    "        \"\"\"\n",
    "        Processing function. This is where the actual analysis happens.\n",
    "        \"\"\"\n",
    "        output = self.accumulator.identity()\n",
    "        dataset = df[\"dataset\"]\n",
    "        cfg = loadConfig()\n",
    "        \n",
    "        ## Muons\n",
    "        muon = Collections(df, \"Muon\", \"tight\").get()\n",
    "        vetomuon = Collections(df, \"Muon\", \"veto\").get()\n",
    "        dimuon = muon.choose(2)\n",
    "        SSmuon = ( dimuon[(dimuon.i0.charge * dimuon.i1.charge)>0].counts>0 )\n",
    "        \n",
    "        ## Electrons\n",
    "        electron = Collections(df, \"Electron\", \"tight\").get()\n",
    "        vetoelectron = Collections(df, \"Electron\", \"veto\").get()\n",
    "        dielectron = electron.choose(2)\n",
    "        SSelectron = ( dielectron[(dielectron.i0.charge * dielectron.i1.charge)>0].counts>0 )\n",
    "\n",
    "        ## E/Mu cross\n",
    "        dilepton = electron.cross(muon)\n",
    "        SSdilepton = ( dilepton[(dilepton.i0.charge * dilepton.i1.charge)>0].counts>0 )\n",
    "        \n",
    "        ## how to get leading lepton easily? Do I actually care?\n",
    "        leading_muon = muon[muon.pt.argmax()]\n",
    "        leading_electron = electron[electron.pt.argmax()]\n",
    "        \n",
    "        lepton = mergeArray(electron, muon)\n",
    "        '''\n",
    "        ok so this is getting **really** awkward (pun slightly intended). because the mergeArray function builds a JaggedArray that has a UnionArry as .content, which in turn\n",
    "        does not work with .argmax(), we need to build a jagged array just holding the pts\n",
    "        '''\n",
    "        lepton_pt = awkward.concatenate([electron.pt, muon.pt], axis=1)\n",
    "        # getting the index of the leading lepton\n",
    "        leading_lep_index = lepton_pt.argmax()\n",
    "        # index of the trailing lepton -> !! if there's only one lepton, it will be the same as the leading one !!\n",
    "        trailing_lep_index = lepton_pt.argmin()\n",
    "\n",
    "        leading_lep_pt = lepton[leading_lep_index].p4.fPt.max() # taking the max here has no impact, but otherwise code fails\n",
    "        leading_lep_eta = lepton[leading_lep_index].p4.fEta.max() # taking the max here has no impact, but otherwise code fails\n",
    "\n",
    "        trailing_lep_pt = lepton[trailing_lep_index].p4.fPt.max() # taking the max here has no impact, but otherwise code fails\n",
    "        trailing_lep_eta = lepton[trailing_lep_index].p4.fEta.max() # taking the max here has no impact, but otherwise code fails\n",
    "\n",
    "        \n",
    "        ## Jets\n",
    "        jet = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['nJet'],\n",
    "            pt = df['Jet_pt'].content,\n",
    "            eta = df['Jet_eta'].content,\n",
    "            phi = df['Jet_phi'].content,\n",
    "            mass = df['Jet_mass'].content,\n",
    "            jetId = df['Jet_jetId'].content, # https://twiki.cern.ch/twiki/bin/view/CMS/JetID\n",
    "            puId = df['Jet_puId'].content, # https://twiki.cern.ch/twiki/bin/viewauth/CMS/PileupJetID\n",
    "            btagDeepB = df['Jet_btagDeepB'].content, # https://twiki.cern.ch/twiki/bin/viewauth/CMS/BtagRecommendation102X\n",
    "        )\n",
    "        \n",
    "        jet       = jet[(jet.pt>25) & (jet.jetId>1)]\n",
    "        jet       = jet[~jet.match(muon, deltaRCut=0.4)] # remove jets that overlap with muons\n",
    "        jet       = jet[~jet.match(electron, deltaRCut=0.4)] # remove jets that overlap with electrons\n",
    "        btag      = jet[(jet.btagDeepB>0.4184) & (abs(jet.eta)<2.4)]\n",
    "        light     = jet[((jet.btagDeepB<0.4184) & (abs(jet.eta)<2.4)) | (abs(jet.eta)>=2.4)]\n",
    "        lightCentral = jet[(jet.btagDeepB<0.4184) & (abs(jet.eta)<2.4) & (jet.pt>30)]\n",
    "        fw        = light[abs(light.eta).argmax()] # the most forward light jet\n",
    "        \n",
    "        ## this is my personal forward jet definition, can be used later\n",
    "        spectator = jet[(abs(jet.eta)>2.0) & (abs(jet.eta)<4.7) & (jet.pt>25) & ( ((jet.puId>6) & (jet.pt<50)) | (jet.pt>50) ) & (jet.jetId>1)] # 40 GeV seemed good. let's try going lower\n",
    "        leading_spectator = spectator[spectator.pt.argmax()]        \n",
    "\n",
    "        leading_jet = jet[jet.pt.argmax()]\n",
    "        leading_b = btag[btag.pt.argmax()]\n",
    "\n",
    "        mass_eb = electron.cross(btag).mass\n",
    "        mass_mub = muon.cross(btag).mass\n",
    "        mass_lb = awkward.concatenate([mass_eb, mass_mub], axis=1)\n",
    "        mlb_min = mass_lb.min()\n",
    "        mlb_max = mass_lb.max()\n",
    "\n",
    "        mll = awkward.concatenate([dimuon.mass, dielectron.mass, dilepton.mass], axis=1).max() # max shouldn't matter, again\n",
    "        ej  = electron.cross(jet)\n",
    "        muj = muon.cross(jet)\n",
    "        deltaR_ej = ej.i0.p4.delta_r(ej.i1.p4)\n",
    "        deltaR_muj = muj.i0.p4.delta_r(muj.i1.p4)\n",
    "\n",
    "        deltaR_lj = awkward.concatenate([deltaR_ej,deltaR_muj], axis=1)\n",
    "        deltaR_lj_min = deltaR_lj.min()\n",
    "        \n",
    "        \n",
    "        ## MET -> can switch to puppi MET\n",
    "        met_pt  = df[\"MET_pt\"]\n",
    "        met_phi = df[\"MET_phi\"]\n",
    "\n",
    "        ## other variables\n",
    "        st = df[\"MET_pt\"] + jet.pt.sum() + muon.pt.sum() + electron.pt.sum()\n",
    "        ht = jet.pt.sum()\n",
    "        \n",
    "        ## Event classifieres\n",
    "        # We want to get the deltaEta between the most forward jet fw and the jet giving the largest invariant mass with fw, fw2\n",
    "        jj = fw.cross(light)\n",
    "        deltaEta = abs(fw.eta - jj[jj.mass.argmax()].i1.eta)\n",
    "        deltaEtaJJMin = ((deltaEta>2).any())\n",
    "        \n",
    "        light_light = light.cross(light)\n",
    "        mjj_max = light_light[light_light.mass.argmax()].mass\n",
    "        \n",
    "        ## define selections (maybe move to a different file at some point)\n",
    "        dilep      = ((electron.counts + muon.counts)==2)\n",
    "        leppt      = (((electron.pt>25).counts + (muon.pt>25).counts)>0)\n",
    "        lepveto    = ((vetoelectron.counts + vetomuon.counts)==2)\n",
    "        SS         = (SSelectron | SSmuon | SSdilepton)\n",
    "        Mll_veto   = (~(dimuon.mass<125).any() & ~(dielectron.mass<125).any() & ~(dilepton.mass<125).any() )\n",
    "        Mll_veto2  = (~(abs(dimuon.mass-91.2)<15).any() & ~(abs(dielectron.mass-91.2)<15).any() ) # not so strict Z veto\n",
    "        nbtag      = (btag.counts>0)\n",
    "        met        = (met_pt > 50)\n",
    "        nfwd       = (spectator.counts>0)\n",
    "        fwdJet50   = ((leading_spectator.pt>50).any())\n",
    "        ptl100     = (((leading_muon.pt>100).any()) | ((leading_electron.pt>100).any()))\n",
    "        eta_fwd    = ((abs(light.eta)>1.75).any())\n",
    "\n",
    "        ## evaluate NN\n",
    "        # first, prepare the inputs.\n",
    "        # A .max() can ensure that the flattened array has the full length, but we rather use our pad_and_flatten function        \n",
    "        # sorting in training: ['mll', 'njet', 'nbtag', 'st', 'ht', 'met', 'mjj_max', 'mlb_min', 'mlb_max', 'l0_pt', 'l1_pt', 'deltaR_lj_min', 'j0_pt']\n",
    "\n",
    "        NN_inputs = np.stack([\n",
    "            # normalize\n",
    "            pad_and_flatten( (mll - self.means['mll'])/self.stds['mll'] ),\n",
    "            pad_and_flatten( (jet.counts - self.means['njet'])/self.stds['njet'] ),\n",
    "            pad_and_flatten( (btag.counts - self.means['nbtag'])/self.stds['nbtag'] ),\n",
    "            pad_and_flatten( (st - self.means['st'])/self.stds['st'] ),\n",
    "            pad_and_flatten( (ht - self.means['ht'])/self.stds['ht'] ),\n",
    "            pad_and_flatten( (met_pt - self.means['met'])/self.stds['met'] ),\n",
    "            pad_and_flatten( (mjj_max - self.means['mjj_max'])/self.stds['mjj_max'] ),\n",
    "            pad_and_flatten( (mlb_min - self.means['mlb_min'])/self.stds['mlb_min'] ),\n",
    "            pad_and_flatten( (mlb_max - self.means['mlb_max'])/self.stds['mlb_max'] ),\n",
    "            pad_and_flatten( (leading_lep_pt - self.means['l0_pt'])/self.stds['l0_pt'] ),\n",
    "            pad_and_flatten( (trailing_lep_pt - self.means['l1_pt'])/self.stds['l1_pt'] ),\n",
    "            pad_and_flatten( (deltaR_lj_min - self.means['deltaR_lj_min'])/self.stds['deltaR_lj_min'] ),\n",
    "            pad_and_flatten( (leading_jet.pt - self.means['j0_pt'])/self.stds['j0_pt'] ),\n",
    "        ])\n",
    "        \n",
    "        NN_inputs = np.moveaxis(NN_inputs, 0, 1)\n",
    "        NN_score = self.model.predict(NN_inputs)\n",
    "        \n",
    "        #NN_score = run_model( NN_inputs )\n",
    "        #global graph\n",
    "        #with graph.as_default():\n",
    "        #    NN_score = model.predict( NN_inputs )\n",
    "        #output['test1'] = NLL_inputs\n",
    "        \n",
    "        output['totalEvents']['all'] += len(df['weight'])\n",
    "        \n",
    "        # Cutflow\n",
    "        processes = ['tW_scattering', 'TTW', 'TTZ', 'TTH', 'TTTT', 'diboson', 'ttbar', 'DY']\n",
    "        cutflow = Cutflow(output, df, cfg, processes)\n",
    "        \n",
    "        cutflow.addRow( 'dilep',       dilep )\n",
    "        cutflow.addRow( 'leppt',       leppt )\n",
    "        cutflow.addRow( 'lepveto',     lepveto )\n",
    "        cutflow.addRow( 'SS',          SS )\n",
    "        \n",
    "        ss_selection = copy.deepcopy(cutflow.selection)\n",
    "        \n",
    "        cutflow.addRow( 'NN>0.5',     (NN_score.flatten()>0.5) )\n",
    "        cutflow.addRow( 'NN>0.55',     (NN_score.flatten()>0.55) )\n",
    "        #cutflow.addRow( 'NN>0.6',     (NN_score.flatten()>0.6) )\n",
    "        #cutflow.addRow( 'NN>0.7',     (NN_score.flatten()>0.7) )\n",
    "        cutflow.addRow( 'njet4',       (jet.counts>=4) )\n",
    "        cutflow.addRow( 'nbtag',       nbtag )\n",
    "        cutflow.addRow( 'central3',    (lightCentral.counts>=3) )\n",
    "        \n",
    "        #cutflow.addRow( 'central4',    (lightCentral.counts>=4) ) # not very efficient for signal. lets look at the plot\n",
    "        \n",
    "        #cutflow.addRow( 'lep100',      ptl100 )\n",
    "        #cutflow.addRow( 'Mll',         Mll_veto2 ) # switched to not so strict Z veto\n",
    "        cutflow.addRow( 'MET>50',      met )\n",
    "        cutflow.addRow( 'njet5',       (jet.counts>=5) )\n",
    "        \n",
    "        baseline = copy.deepcopy(cutflow.selection)\n",
    "        \n",
    "        cutflow.addRow( 'eta_fwd',     eta_fwd )\n",
    "        cutflow.addRow( 'deltaEtaJJ',  deltaEtaJJMin )\n",
    "        cutflow.addRow( 'ST>500',      (st>500) )\n",
    "        cutflow.addRow( 'ST>600',      (st>600) )\n",
    "        cutflow.addRow( 'nfwd',        nfwd )\n",
    "        cutflow.addRow( 'fwdJet50',    fwdJet50 )\n",
    "\n",
    "        # preselection of events\n",
    "        event_selection = cutflow.selection\n",
    "        \n",
    "        ### And fill the histograms\n",
    "        # just the number of electrons and muons\n",
    "        output['N_ele'].fill(dataset=dataset, multiplicity=electron[event_selection].counts, weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "        output['N_mu'].fill(dataset=dataset, multiplicity=muon[event_selection].counts, weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "        ## N jet and N b without selections on those\n",
    "        output['N_jet'].fill(dataset=dataset, multiplicity=jet[dilep].counts, weight=df['weight'][dilep]*cfg['lumi'])\n",
    "        output['N_b'].fill(dataset=dataset, multiplicity=btag[dilep].counts, weight=df['weight'][dilep]*cfg['lumi'])\n",
    "        ## forward jet properties\n",
    "        output['N_spec'].fill(dataset=dataset, multiplicity=spectator[event_selection].counts, weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "        output['pt_spec_max'].fill(dataset=dataset, pt=leading_spectator[event_selection].pt.flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "        output['eta_spec_max'].fill(dataset=dataset, eta=leading_spectator[event_selection].eta.flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "        \n",
    "        ## something a bit more tricky\n",
    "        output['N_diele'].fill(dataset=dataset, multiplicity=dielectron[event_selection].counts, weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "        output['N_dimu'].fill(dataset=dataset, multiplicity=dimuon[event_selection].counts, weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "\n",
    "        output['mjj_max'].fill(dataset=dataset, mass=mjj_max[event_selection].flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "        #output['mjj_max_baseline'].fill(dataset=dataset, mass=mjj_max[baseline].flatten(), weight=df['weight'][baseline]*cfg['lumi'])\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c4b1506b1fe4eb684718f5d3ca18015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Preprocessing', max=140.0, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44aa60f060d6401b808e7fd28bb3ce62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Processing', max=1170.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "overwrite = True\n",
    "small = False\n",
    "\n",
    "# load the config and the cache\n",
    "cfg = loadConfig()\n",
    "\n",
    "cacheName = 'strong_tW_small' if small else 'strong_tW'\n",
    "\n",
    "from samples import fileset, fileset_small, fileset_2l, fileset_SS\n",
    "\n",
    "# histograms\n",
    "histograms = []\n",
    "histograms += ['N_ele', 'N_mu', 'N_diele', 'N_dimu', 'mjj_max', 'mjj_max_baseline', 'pt_spec_max', 'eta_spec_max']\n",
    "\n",
    "# initialize cache\n",
    "cache = dir_archive(os.path.join(os.path.expandvars(cfg['caches']['base']), cfg['caches'][cacheName]), serialized=True)\n",
    "if not overwrite:\n",
    "    cache.load()\n",
    "\n",
    "if cfg == cache.get('cfg') and histograms == cache.get('histograms') and cache.get('simple_output'):\n",
    "    output = cache.get('simple_output')\n",
    "\n",
    "else:\n",
    "    # Run the processor\n",
    "    if small:\n",
    "        #fileset = {'tW_scattering':fileset_small['tW_scattering'][:4]}#, 'ttbar':fileset_small['ttbar'][:1]}\n",
    "        fileset = {'TTW':fileset_small['TTW'][:1], 'ttbar':fileset_small['ttbar'][:1]}\n",
    "        workers = 4\n",
    "    else:\n",
    "        fileset = fileset_SS\n",
    "        #fileset = {'TTW': fileset_SS['TTW'], 'ttbar':fileset_SS['ttbar'], 'tW_scattering':fileset_SS['tW_scattering']}\n",
    "        workers = 8\n",
    "    output = processor.run_uproot_job(fileset,\n",
    "                                      treename='Events',\n",
    "                                      processor_instance=analysisProcessor(),\n",
    "                                      executor=processor.futures_executor,\n",
    "                                      executor_args={'workers': workers, 'function_args': {'flatten': False}},\n",
    "                                      chunksize=100000,\n",
    "                                     )\n",
    "    cache['fileset']        = fileset\n",
    "    cache['cfg']            = cfg\n",
    "    cache['histograms']     = histograms\n",
    "    cache['simple_output']  = output\n",
    "    cache.dump()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the keras model had several problems:\n",
    "```\n",
    "ValueError: Tensor Tensor(\"dense_2/Sigmoid:0\", shape=(?, 1), dtype=float32) is not an element of this graph.\n",
    "TypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"dense_1_input_1:0\", shape=(?, 13), dtype=float32) is not an element of this graph.\n",
    "```\n",
    "--> These are problems coming from the fact that the processor runs in a different thread than loading the model.\n",
    "It should be possible to solve those issues by \n",
    "```\n",
    "model = tf.keras.models.load_model('../ML/data/training.h5')#, custom_objects=None, compile=False)\n",
    "\n",
    "model._make_predict_function()\n",
    "graph = tf.get_default_graph()\n",
    "```\n",
    "and then, when using predict\n",
    "```\n",
    "global graph\n",
    "with graph.as_default():\n",
    "    outputs = model.predict(inputs)\n",
    "```\n",
    "However, the errors persisted.\n",
    "If we use theano instead of tensorflow, it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram = output['eta_spec_max']\n",
    "ax = hist.plot1d(histogram,overlay=\"dataset\", stack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "histogram = output['mjj_max_baseline']\n",
    "ax = hist.plot1d(histogram['ttbar'],overlay=\"dataset\", stack=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram = output['mjj_max']\n",
    "ax = hist.plot1d(histogram['ttbar'],overlay=\"dataset\", stack=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tW_scattering</th>\n",
       "      <th>TTW</th>\n",
       "      <th>TTH</th>\n",
       "      <th>TTZ</th>\n",
       "      <th>TTTT</th>\n",
       "      <th>diboson</th>\n",
       "      <th>ttbar</th>\n",
       "      <th>DY</th>\n",
       "      <th>S/B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>entry</th>\n",
       "      <td>2307.0 +/- 18.0</td>\n",
       "      <td>27870.0 +/- 60.0</td>\n",
       "      <td>19880.0 +/- 10.0</td>\n",
       "      <td>24220.0 +/- 20.0</td>\n",
       "      <td>558.5 +/- 0.7</td>\n",
       "      <td>1083000.0 +/- 1000.0</td>\n",
       "      <td>16640000.0 +/- 0.0</td>\n",
       "      <td>22780000.0 +/- 10000.0</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dilep</th>\n",
       "      <td>380.3 +/- 7.2</td>\n",
       "      <td>4489.0 +/- 18.0</td>\n",
       "      <td>2930.0 +/- 5.0</td>\n",
       "      <td>8678.0 +/- 10.0</td>\n",
       "      <td>122.7 +/- 0.3</td>\n",
       "      <td>183300.0 +/- 200.0</td>\n",
       "      <td>1456000.0 +/- 1000.0</td>\n",
       "      <td>12610000.0 +/- 10000.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leppt</th>\n",
       "      <td>380.3 +/- 7.2</td>\n",
       "      <td>4489.0 +/- 18.0</td>\n",
       "      <td>2930.0 +/- 5.0</td>\n",
       "      <td>8678.0 +/- 10.0</td>\n",
       "      <td>122.7 +/- 0.3</td>\n",
       "      <td>183300.0 +/- 200.0</td>\n",
       "      <td>1456000.0 +/- 1000.0</td>\n",
       "      <td>12610000.0 +/- 10000.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lepveto</th>\n",
       "      <td>279.8 +/- 6.2</td>\n",
       "      <td>3435.0 +/- 16.0</td>\n",
       "      <td>2085.0 +/- 4.0</td>\n",
       "      <td>5961.0 +/- 9.0</td>\n",
       "      <td>72.72 +/- 0.26</td>\n",
       "      <td>146500.0 +/- 100.0</td>\n",
       "      <td>1224000.0 +/- 1000.0</td>\n",
       "      <td>10800000.0 +/- 10000.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SS</th>\n",
       "      <td>94.19 +/- 3.6</td>\n",
       "      <td>1125.0 +/- 5.0</td>\n",
       "      <td>535.5 +/- 1.8</td>\n",
       "      <td>484.3 +/- 2.4</td>\n",
       "      <td>23.97 +/- 0.15</td>\n",
       "      <td>3926.0 +/- 34.0</td>\n",
       "      <td>5630.0 +/- 54.0</td>\n",
       "      <td>4382.0 +/- 190.0</td>\n",
       "      <td>0.0058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN&gt;0.5</th>\n",
       "      <td>52.76 +/- 2.72</td>\n",
       "      <td>617.3 +/- 3.5</td>\n",
       "      <td>253.1 +/- 1.3</td>\n",
       "      <td>209.2 +/- 1.5</td>\n",
       "      <td>21.73 +/- 0.14</td>\n",
       "      <td>62.65 +/- 4.06</td>\n",
       "      <td>988.1 +/- 19.0</td>\n",
       "      <td>24.72 +/- 14.27</td>\n",
       "      <td>0.0242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN&gt;0.55</th>\n",
       "      <td>48.0 +/- 2.6</td>\n",
       "      <td>555.8 +/- 3.3</td>\n",
       "      <td>223.3 +/- 1.2</td>\n",
       "      <td>184.4 +/- 1.4</td>\n",
       "      <td>21.04 +/- 0.14</td>\n",
       "      <td>52.44 +/- 3.67</td>\n",
       "      <td>796.4 +/- 16.9</td>\n",
       "      <td>24.72 +/- 14.27</td>\n",
       "      <td>0.0258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN&gt;0.7</th>\n",
       "      <td>0 +/- 0.0</td>\n",
       "      <td>0 +/- 0.0</td>\n",
       "      <td>0 +/- 0.0</td>\n",
       "      <td>0 +/- 0.0</td>\n",
       "      <td>0 +/- 0.0</td>\n",
       "      <td>0 +/- 0.0</td>\n",
       "      <td>0 +/- 0.0</td>\n",
       "      <td>0 +/- 0.0</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>njet4</th>\n",
       "      <td>45.04 +/- 2.48</td>\n",
       "      <td>425.8 +/- 3.0</td>\n",
       "      <td>187.3 +/- 1.1</td>\n",
       "      <td>154.1 +/- 1.3</td>\n",
       "      <td>20.93 +/- 0.14</td>\n",
       "      <td>26.32 +/- 2.51</td>\n",
       "      <td>552.5 +/- 13.8</td>\n",
       "      <td>24.72 +/- 14.27</td>\n",
       "      <td>0.0324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nbtag</th>\n",
       "      <td>45.04 +/- 2.48</td>\n",
       "      <td>425.8 +/- 3.0</td>\n",
       "      <td>187.3 +/- 1.1</td>\n",
       "      <td>154.1 +/- 1.3</td>\n",
       "      <td>20.93 +/- 0.14</td>\n",
       "      <td>26.32 +/- 2.51</td>\n",
       "      <td>552.5 +/- 13.8</td>\n",
       "      <td>24.72 +/- 14.27</td>\n",
       "      <td>0.0324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>central3</th>\n",
       "      <td>28.44 +/- 1.95</td>\n",
       "      <td>205.5 +/- 2.2</td>\n",
       "      <td>109.3 +/- 0.9</td>\n",
       "      <td>81.78 +/- 1.01</td>\n",
       "      <td>16.9 +/- 0.13</td>\n",
       "      <td>12.09 +/- 1.64</td>\n",
       "      <td>272.9 +/- 9.4</td>\n",
       "      <td>0 +/- 0.0</td>\n",
       "      <td>0.0407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MET&gt;50</th>\n",
       "      <td>23.93 +/- 1.78</td>\n",
       "      <td>179.9 +/- 2.1</td>\n",
       "      <td>95.13 +/- 0.8</td>\n",
       "      <td>66.79 +/- 0.91</td>\n",
       "      <td>14.64 +/- 0.12</td>\n",
       "      <td>10.39 +/- 1.54</td>\n",
       "      <td>219.4 +/- 8.4</td>\n",
       "      <td>0 +/- 0.0</td>\n",
       "      <td>0.0408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>njet5</th>\n",
       "      <td>22.52 +/- 1.71</td>\n",
       "      <td>147.7 +/- 1.9</td>\n",
       "      <td>84.87 +/- 0.75</td>\n",
       "      <td>59.09 +/- 0.87</td>\n",
       "      <td>14.56 +/- 0.12</td>\n",
       "      <td>9.208 +/- 1.455</td>\n",
       "      <td>182.7 +/- 7.5</td>\n",
       "      <td>0 +/- 0.0</td>\n",
       "      <td>0.0452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>central4</th>\n",
       "      <td>0 +/- 0.0</td>\n",
       "      <td>0 +/- 0.0</td>\n",
       "      <td>0 +/- 0.0</td>\n",
       "      <td>0 +/- 0.0</td>\n",
       "      <td>0 +/- 0.0</td>\n",
       "      <td>0 +/- 0.0</td>\n",
       "      <td>0 +/- 0.0</td>\n",
       "      <td>0 +/- 0.0</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eta_fwd</th>\n",
       "      <td>19.17 +/- 1.58</td>\n",
       "      <td>96.53 +/- 1.55</td>\n",
       "      <td>55.99 +/- 0.61</td>\n",
       "      <td>42.08 +/- 0.74</td>\n",
       "      <td>10.8 +/- 0.1</td>\n",
       "      <td>8.009 +/- 1.368</td>\n",
       "      <td>139.3 +/- 6.6</td>\n",
       "      <td>0 +/- 0.0</td>\n",
       "      <td>0.0544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deltaEtaJJ</th>\n",
       "      <td>17.89 +/- 1.53</td>\n",
       "      <td>77.97 +/- 1.4</td>\n",
       "      <td>46.12 +/- 0.56</td>\n",
       "      <td>35.35 +/- 0.67</td>\n",
       "      <td>9.534 +/- 0.096</td>\n",
       "      <td>7.052 +/- 1.3</td>\n",
       "      <td>115.5 +/- 6.1</td>\n",
       "      <td>0 +/- 0.0</td>\n",
       "      <td>0.0614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ST&gt;500</th>\n",
       "      <td>17.89 +/- 1.53</td>\n",
       "      <td>76.43 +/- 1.39</td>\n",
       "      <td>45.05 +/- 0.55</td>\n",
       "      <td>34.72 +/- 0.67</td>\n",
       "      <td>9.524 +/- 0.096</td>\n",
       "      <td>6.742 +/- 1.262</td>\n",
       "      <td>111.9 +/- 6.1</td>\n",
       "      <td>0 +/- 0.0</td>\n",
       "      <td>0.0629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ST&gt;600</th>\n",
       "      <td>17.11 +/- 1.5</td>\n",
       "      <td>70.39 +/- 1.34</td>\n",
       "      <td>40.63 +/- 0.53</td>\n",
       "      <td>31.83 +/- 0.64</td>\n",
       "      <td>9.373 +/- 0.096</td>\n",
       "      <td>5.854 +/- 1.168</td>\n",
       "      <td>98.53 +/- 5.76</td>\n",
       "      <td>0 +/- 0.0</td>\n",
       "      <td>0.0667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nfwd</th>\n",
       "      <td>13.38 +/- 1.31</td>\n",
       "      <td>47.74 +/- 1.09</td>\n",
       "      <td>29.04 +/- 0.45</td>\n",
       "      <td>23.01 +/- 0.55</td>\n",
       "      <td>7.31 +/- 0.084</td>\n",
       "      <td>5.254 +/- 1.097</td>\n",
       "      <td>67.49 +/- 4.84</td>\n",
       "      <td>0 +/- 0.0</td>\n",
       "      <td>0.0744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwdJet50</th>\n",
       "      <td>11.2 +/- 1.2</td>\n",
       "      <td>32.11 +/- 0.89</td>\n",
       "      <td>18.99 +/- 0.36</td>\n",
       "      <td>16.11 +/- 0.46</td>\n",
       "      <td>5.225 +/- 0.072</td>\n",
       "      <td>3.47 +/- 0.928</td>\n",
       "      <td>46.5 +/- 4.25</td>\n",
       "      <td>0 +/- 0.0</td>\n",
       "      <td>0.0915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tW_scattering               TTW               TTH  \\\n",
       "entry       2307.0 +/- 18.0  27870.0 +/- 60.0  19880.0 +/- 10.0   \n",
       "dilep         380.3 +/- 7.2   4489.0 +/- 18.0    2930.0 +/- 5.0   \n",
       "leppt         380.3 +/- 7.2   4489.0 +/- 18.0    2930.0 +/- 5.0   \n",
       "lepveto       279.8 +/- 6.2   3435.0 +/- 16.0    2085.0 +/- 4.0   \n",
       "SS            94.19 +/- 3.6    1125.0 +/- 5.0     535.5 +/- 1.8   \n",
       "NN>0.5       52.76 +/- 2.72     617.3 +/- 3.5     253.1 +/- 1.3   \n",
       "NN>0.55        48.0 +/- 2.6     555.8 +/- 3.3     223.3 +/- 1.2   \n",
       "NN>0.7            0 +/- 0.0         0 +/- 0.0         0 +/- 0.0   \n",
       "njet4        45.04 +/- 2.48     425.8 +/- 3.0     187.3 +/- 1.1   \n",
       "nbtag        45.04 +/- 2.48     425.8 +/- 3.0     187.3 +/- 1.1   \n",
       "central3     28.44 +/- 1.95     205.5 +/- 2.2     109.3 +/- 0.9   \n",
       "MET>50       23.93 +/- 1.78     179.9 +/- 2.1     95.13 +/- 0.8   \n",
       "njet5        22.52 +/- 1.71     147.7 +/- 1.9    84.87 +/- 0.75   \n",
       "central4          0 +/- 0.0         0 +/- 0.0         0 +/- 0.0   \n",
       "eta_fwd      19.17 +/- 1.58    96.53 +/- 1.55    55.99 +/- 0.61   \n",
       "deltaEtaJJ   17.89 +/- 1.53     77.97 +/- 1.4    46.12 +/- 0.56   \n",
       "ST>500       17.89 +/- 1.53    76.43 +/- 1.39    45.05 +/- 0.55   \n",
       "ST>600        17.11 +/- 1.5    70.39 +/- 1.34    40.63 +/- 0.53   \n",
       "nfwd         13.38 +/- 1.31    47.74 +/- 1.09    29.04 +/- 0.45   \n",
       "fwdJet50       11.2 +/- 1.2    32.11 +/- 0.89    18.99 +/- 0.36   \n",
       "\n",
       "                         TTZ             TTTT               diboson  \\\n",
       "entry       24220.0 +/- 20.0    558.5 +/- 0.7  1083000.0 +/- 1000.0   \n",
       "dilep        8678.0 +/- 10.0    122.7 +/- 0.3    183300.0 +/- 200.0   \n",
       "leppt        8678.0 +/- 10.0    122.7 +/- 0.3    183300.0 +/- 200.0   \n",
       "lepveto       5961.0 +/- 9.0   72.72 +/- 0.26    146500.0 +/- 100.0   \n",
       "SS             484.3 +/- 2.4   23.97 +/- 0.15       3926.0 +/- 34.0   \n",
       "NN>0.5         209.2 +/- 1.5   21.73 +/- 0.14        62.65 +/- 4.06   \n",
       "NN>0.55        184.4 +/- 1.4   21.04 +/- 0.14        52.44 +/- 3.67   \n",
       "NN>0.7             0 +/- 0.0        0 +/- 0.0             0 +/- 0.0   \n",
       "njet4          154.1 +/- 1.3   20.93 +/- 0.14        26.32 +/- 2.51   \n",
       "nbtag          154.1 +/- 1.3   20.93 +/- 0.14        26.32 +/- 2.51   \n",
       "central3      81.78 +/- 1.01    16.9 +/- 0.13        12.09 +/- 1.64   \n",
       "MET>50        66.79 +/- 0.91   14.64 +/- 0.12        10.39 +/- 1.54   \n",
       "njet5         59.09 +/- 0.87   14.56 +/- 0.12       9.208 +/- 1.455   \n",
       "central4           0 +/- 0.0        0 +/- 0.0             0 +/- 0.0   \n",
       "eta_fwd       42.08 +/- 0.74     10.8 +/- 0.1       8.009 +/- 1.368   \n",
       "deltaEtaJJ    35.35 +/- 0.67  9.534 +/- 0.096         7.052 +/- 1.3   \n",
       "ST>500        34.72 +/- 0.67  9.524 +/- 0.096       6.742 +/- 1.262   \n",
       "ST>600        31.83 +/- 0.64  9.373 +/- 0.096       5.854 +/- 1.168   \n",
       "nfwd          23.01 +/- 0.55   7.31 +/- 0.084       5.254 +/- 1.097   \n",
       "fwdJet50      16.11 +/- 0.46  5.225 +/- 0.072        3.47 +/- 0.928   \n",
       "\n",
       "                           ttbar                      DY     S/B  \n",
       "entry         16640000.0 +/- 0.0  22780000.0 +/- 10000.0  0.0001  \n",
       "dilep       1456000.0 +/- 1000.0  12610000.0 +/- 10000.0  0.0000  \n",
       "leppt       1456000.0 +/- 1000.0  12610000.0 +/- 10000.0  0.0000  \n",
       "lepveto     1224000.0 +/- 1000.0  10800000.0 +/- 10000.0  0.0000  \n",
       "SS               5630.0 +/- 54.0        4382.0 +/- 190.0  0.0058  \n",
       "NN>0.5            988.1 +/- 19.0         24.72 +/- 14.27  0.0242  \n",
       "NN>0.55           796.4 +/- 16.9         24.72 +/- 14.27  0.0258  \n",
       "NN>0.7                 0 +/- 0.0               0 +/- 0.0  1.0000  \n",
       "njet4             552.5 +/- 13.8         24.72 +/- 14.27  0.0324  \n",
       "nbtag             552.5 +/- 13.8         24.72 +/- 14.27  0.0324  \n",
       "central3           272.9 +/- 9.4               0 +/- 0.0  0.0407  \n",
       "MET>50             219.4 +/- 8.4               0 +/- 0.0  0.0408  \n",
       "njet5              182.7 +/- 7.5               0 +/- 0.0  0.0452  \n",
       "central4               0 +/- 0.0               0 +/- 0.0  1.0000  \n",
       "eta_fwd            139.3 +/- 6.6               0 +/- 0.0  0.0544  \n",
       "deltaEtaJJ         115.5 +/- 6.1               0 +/- 0.0  0.0614  \n",
       "ST>500             111.9 +/- 6.1               0 +/- 0.0  0.0629  \n",
       "ST>600            98.53 +/- 5.76               0 +/- 0.0  0.0667  \n",
       "nfwd              67.49 +/- 4.84               0 +/- 0.0  0.0744  \n",
       "fwdJet50           46.5 +/- 4.25               0 +/- 0.0  0.0915  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cutflow\n",
    "processes = ['tW_scattering','TTW', 'TTH', 'TTZ', 'TTTT', 'diboson','ttbar', 'DY']\n",
    "lines = ['entry']\n",
    "lines += ['dilep', 'leppt', 'lepveto', 'SS','NN>0.5','NN>0.55','NN>0.7', 'njet4', 'nbtag', 'central3', 'MET>50', 'njet5', 'central4',   'eta_fwd', 'deltaEtaJJ', 'ST>500', 'ST>600' ]\n",
    "lines += ['nfwd', 'fwdJet50'] # those are my cuts\n",
    "df = getCutFlowTable(output, processes=processes, lines=lines, significantFigures=4, signal='tW_scattering')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tW_scattering</th>\n",
       "      <th>TTW</th>\n",
       "      <th>TTH</th>\n",
       "      <th>TTZ</th>\n",
       "      <th>TTTT</th>\n",
       "      <th>diboson</th>\n",
       "      <th>ttbar</th>\n",
       "      <th>DY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>entry</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dilep</th>\n",
       "      <td>0.165</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leppt</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lepveto</th>\n",
       "      <td>0.736</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SS</th>\n",
       "      <td>0.337</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN&gt;0.5</th>\n",
       "      <td>0.560</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN&gt;0.55</th>\n",
       "      <td>0.910</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.806</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN&gt;0.7</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>njet4</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nbtag</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>central3</th>\n",
       "      <td>0.631</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MET&gt;50</th>\n",
       "      <td>0.842</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.804</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>njet5</th>\n",
       "      <td>0.941</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.833</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>central4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eta_fwd</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deltaEtaJJ</th>\n",
       "      <td>0.933</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0.829</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ST&gt;500</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.969</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ST&gt;600</th>\n",
       "      <td>0.957</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.917</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.880</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nfwd</th>\n",
       "      <td>0.782</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.685</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwdJet50</th>\n",
       "      <td>0.837</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.689</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tW_scattering    TTW    TTH    TTZ   TTTT  diboson  ttbar     DY\n",
       "entry               1.000  1.000  1.000  1.000  1.000    1.000  1.000  1.000\n",
       "dilep               0.165  0.161  0.147  0.358  0.220    0.169  0.087  0.554\n",
       "leppt               1.000  1.000  1.000  1.000  1.000    1.000  1.000  1.000\n",
       "lepveto             0.736  0.765  0.712  0.687  0.593    0.799  0.841  0.856\n",
       "SS                  0.337  0.328  0.257  0.081  0.330    0.027  0.005  0.000\n",
       "NN>0.5              0.560  0.549  0.473  0.432  0.906    0.016  0.176  0.006\n",
       "NN>0.55             0.910  0.900  0.882  0.881  0.968    0.837  0.806  1.000\n",
       "NN>0.7              0.000  0.000  0.000  0.000  0.000    0.000  0.000  0.000\n",
       "njet4               1.000  1.000  1.000  1.000  1.000    1.000  1.000  1.000\n",
       "nbtag               1.000  1.000  1.000  1.000  1.000    1.000  1.000  1.000\n",
       "central3            0.631  0.483  0.584  0.531  0.808    0.460  0.494  0.000\n",
       "MET>50              0.842  0.876  0.870  0.817  0.866    0.859  0.804  1.000\n",
       "njet5               0.941  0.821  0.892  0.885  0.994    0.886  0.833  1.000\n",
       "central4            0.000  0.000  0.000  0.000  0.000    0.000  0.000  1.000\n",
       "eta_fwd             1.000  1.000  1.000  1.000  1.000    1.000  1.000  1.000\n",
       "deltaEtaJJ          0.933  0.808  0.824  0.840  0.883    0.881  0.829  1.000\n",
       "ST>500              1.000  0.980  0.977  0.982  0.999    0.956  0.969  1.000\n",
       "ST>600              0.957  0.921  0.902  0.917  0.984    0.868  0.880  1.000\n",
       "nfwd                0.782  0.678  0.715  0.723  0.780    0.897  0.685  1.000\n",
       "fwdJet50            0.837  0.672  0.654  0.700  0.715    0.660  0.689  1.000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Efficiencies\n",
    "df = getCutFlowTable(output, processes=processes, lines=lines, significantFigures=3, absolute=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can probably scale up the signal x-sec by ~1.5 (assuming 12% of the inclusive ttW x-sec, compared to the x-sec we currently use 0.12\\*0.61/0.0478).\n",
    "This increases S/B to about 0.10, with ttbar still being the largest contribution of almost 100 events.\n",
    "Can we use a tighter ID (e.g. ttH/V leptonMVA), or are we missing something?\n",
    "\n",
    "The theory paper has a lower ttbar rate, but probably a tighter lepton ID (similar signal yield for 300/fb as we have for 137/fb).\n",
    "\n",
    "We should check:\n",
    "- which ttX contribution is the largest\n",
    "- use ttH lepton ID -> should be easy to implement now\n",
    "- add some plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots.helpers import *\n",
    "bins = {\\\n",
    "    'N_jet':    {'axis': 'multiplicity',  'overflow':'over',  'bins': hist.Bin('multiplicity', r'$N_{jet}$', 15, -0.5, 14.5)},\n",
    "    'N_spec':   {'axis': 'multiplicity',  'overflow':'over',  'bins': hist.Bin('multiplicity', r'$N_{jet, fwd}$', 6, -0.5, 5.5)},\n",
    "    'N_b':      {'axis': 'multiplicity',  'overflow':'over',  'bins': hist.Bin('multiplicity', r'$N_{b-jet}$', 5, -0.5, 4.5)},\n",
    "    'pt_spec_max': {'axis': 'pt',         'overflow':'over',  'bins': hist.Bin('pt', r'$p_{T, fwd jet}\\ (GeV)$', 20, 0, 400)},\n",
    "    'eta_spec_max': {'axis': 'eta',         'overflow':'over',  'bins': hist.Bin('eta', r'$p\\eta_{fwd jet}\\ (GeV)$', 15, -5.5, 5.5)},\n",
    "    'mjj_max':  {'axis': 'mass',          'overflow':'over',  'bins': hist.Bin('mass', r'$M(jj) \\ (GeV)$', 10, 0, 2000)},\n",
    "    'mjj_max_baseline':  {'axis': 'mass',          'overflow':'over',  'bins': hist.Bin('mass', r'$M(jj) \\ (GeV)$', 10, 0, 2000)},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if output['mjj_max']['pseudodata'].values():\n",
    "    print (\"has it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some of the plots. If they look weird, rerun the notebook again - we use a mplhep style for the NLL scan below.\n",
    "\n",
    "plotDir = os.path.expandvars(cfg['meta']['plots']) + '/dump/'\n",
    "finalizePlotDir(plotDir)\n",
    "\n",
    "from plots.helpers import *\n",
    "\n",
    "if True:\n",
    "    name = 'mjj_max_baseline'\n",
    "    #print (name)\n",
    "    skip = False\n",
    "    \n",
    "    # load the cached results\n",
    "    #output = cache.get('simple_output')\n",
    "    histogram = output[name]\n",
    "\n",
    "    axis = bins[name]['axis']\n",
    "    #print (name, axis)\n",
    "    histogram = histogram.rebin(axis, bins[name]['bins'])\n",
    "\n",
    "    y_max = histogram.sum(\"dataset\").values(overflow='all')[()].max()\n",
    "    y_over = histogram.sum(\"dataset\").values(overflow='all')[()][-1]\n",
    "\n",
    "    # get pseudo data\n",
    "    if not histogram['pseudodata'].values():\n",
    "        print (\"Getting pseudo-data\")\n",
    "        bin_values = histogram.axis(axis).centers(overflow=bins[name]['overflow'])\n",
    "        poisson_means = histogram.sum('dataset').values(overflow=bins[name]['overflow'])[()]\n",
    "        values = np.repeat(bin_values, np.random.poisson(np.maximum(np.zeros(len(poisson_means)), poisson_means)))\n",
    "        if axis == 'pt':\n",
    "            histogram.fill(dataset='pseudodata', pt=values)\n",
    "        elif axis == 'eta':\n",
    "            histogram.fill(dataset='pseudodata', eta=values)\n",
    "        elif axis == 'mass':\n",
    "            histogram.fill(dataset='pseudodata', mass=values)\n",
    "        elif axis == 'multiplicity':\n",
    "            histogram.fill(dataset='pseudodata', multiplicity=values)\n",
    "        elif axis == 'ht':\n",
    "            histogram.fill(dataset='pseudodata', ht=values)\n",
    "        elif axis == 'norm':\n",
    "            histogram.fill(dataset='pseudodata', norm=values)\n",
    "        \n",
    "        # update the cache\n",
    "        output[name] = histogram\n",
    "        #cache['simple_output']  = output\n",
    "        #cache.dump()\n",
    "        \n",
    "    else:\n",
    "        print (\"Pseudo-data found in cache\")\n",
    "\n",
    "    \n",
    "    import re\n",
    "    bkgonly = re.compile('(?!pseudodata|!tW_scattering)')\n",
    "    notdata = re.compile('(?!pseudodata)')\n",
    "    notsignal = re.compile('(?!tW_scattering)')\n",
    "\n",
    "    fig, (ax, rax) = plt.subplots(2, 1, figsize=(7,7), gridspec_kw={\"height_ratios\": (3, 1)}, sharex=True)\n",
    "    #fig, ax = plt.subplots(1,1,figsize=(7,7))\n",
    "    \n",
    "    # get axes\n",
    "    hist.plot1d(histogram[bkgonly],overlay=\"dataset\", ax=ax, stack=True, overflow=bins[name]['overflow'], clear=False, line_opts=None, fill_opts=fill_opts, error_opts=error_opts, order=['TTZ', 'TTH', 'TTTT', 'TTW','ttbar','diboson', 'DY']) #error_opts??\n",
    "    hist.plot1d(histogram['tW_scattering'], overlay=\"dataset\", ax=ax, overflow=bins[name]['overflow'], line_opts={'linewidth':3}, clear=False)\n",
    "    hist.plot1d(histogram['pseudodata'], overlay=\"dataset\", ax=ax, overflow=bins[name]['overflow'], error_opts=data_err_opts, clear=False)\n",
    "\n",
    "    ## build ratio\n",
    "    hist.plotratio(\n",
    "        num=histogram['pseudodata'].sum(\"dataset\"),\n",
    "        denom=histogram[bkgonly].sum(\"dataset\"),\n",
    "        ax=rax,\n",
    "        error_opts=data_err_opts,\n",
    "        denom_fill_opts={},\n",
    "        guide_opts={},\n",
    "        unc='num',\n",
    "        overflow=bins[name]['overflow']\n",
    "    )\n",
    "\n",
    "\n",
    "    for l in ['linear']:\n",
    "        saveFig(fig, ax, None, plotDir, name, scale=l, shape=False, y_max=y_max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make some data cards and run the likelihood fit for the M(jj) distribution\n",
    "\n",
    "As a test we will use the 10 bins of the M(jj) distribution to extract a likelihood ratio. We should be able to find a better variable than M(jj), this is more a proof of concept.\n",
    "\n",
    "First, we extract the histograms from the processor output and store them in a root file. Then, we create a data card, add some mock-up systematic uncertainties and run the fit with combine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tools.dataCard import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the histograms to write into a root file, which subsequently is picked up by combine\n",
    "# in order to also get the content of the overflow bins we use a private version of export1d\n",
    "\n",
    "from Tools.helpers import export1d\n",
    "\n",
    "def makeCardFromHist(hist_name, nonprompt_scale=1, signal_scale=1, bkg_scale=1, overflow='all'):\n",
    "    print (\"Writing cards using histogram:\", hist_name)\n",
    "    card_dir = os.path.expandvars('$TWHOME/data/cards/')\n",
    "    if not os.path.isdir(card_dir):\n",
    "        os.makedirs(card_dir)\n",
    "    \n",
    "    data_card = card_dir+hist_name+'_card.txt'\n",
    "    shape_file = card_dir+hist_name+'_shapes.root'\n",
    "    \n",
    "    histogram = output[hist_name]\n",
    "    histogram = histogram.rebin('mass', bins[hist_name]['bins'])\n",
    "    \n",
    "    # scale some processes\n",
    "    scales = { \n",
    "        'ttbar': nonprompt_scale, \n",
    "        'tW_scattering': signal_scale,\n",
    "        'TTW': bkg_scale, # only scale the most important backgrounds\n",
    "        'TTZ': bkg_scale,\n",
    "        'TTH': bkg_scale,\n",
    "    }\n",
    "    histogram.scale(scales, axis='dataset')\n",
    "    \n",
    "    #observation = hist.export1d(histogram['pseudodata'].integrate('dataset'), overflow=overflow)\n",
    "    observation = export1d(histogram[notdata].integrate('dataset'), overflow=overflow)\n",
    "    tw          = export1d(histogram['tW_scattering'].integrate('dataset'), overflow=overflow)\n",
    "    onlyttx     = re.compile('(TTW|TTZ|TTH|TTTT|diboson|DY)')\n",
    "    bkg         = export1d(histogram[onlyttx].integrate('dataset'), overflow=overflow)\n",
    "    nonprompt   = export1d(histogram['ttbar'].integrate('dataset'), overflow=overflow)\n",
    "    \n",
    "    file = uproot.recreate(shape_file, compression=uproot.ZLIB(4))\n",
    "    \n",
    "    file[\"signal\"]    = tw\n",
    "    file[\"nonprompt\"] = nonprompt\n",
    "    file[\"bkg\"]       = bkg\n",
    "    file[\"data_obs\"]  = observation\n",
    "    \n",
    "    # Get the total yields to write into a data card\n",
    "    totals = {}\n",
    "    \n",
    "    totals['signal']      = histogram['tW_scattering'].integrate('dataset').values(overflow=overflow)[()].sum()\n",
    "    totals['bkg']         = histogram[onlyttx].integrate('dataset').values(overflow=overflow)[()].sum()\n",
    "    totals['nonprompt']   = histogram['ttbar'].integrate('dataset').values(overflow=overflow)[()].sum()\n",
    "    #totals['observation'] = histogram['pseudodata'].integrate('dataset').values(overflow=overflow)[()].sum()\n",
    "    totals['observation'] = histogram[notdata].integrate('dataset').values(overflow=overflow)[()].sum()\n",
    "    \n",
    "    print (\"{:30}{:.2f}\".format(\"Signal expectation:\",totals['signal']) )\n",
    "    print (\"{:30}{:.2f}\".format(\"Non-prompt background:\",totals['nonprompt']) )\n",
    "    print (\"{:30}{:.2f}\".format(\"t(t)X(X)/rare background:\",totals['bkg']) )\n",
    "    print (\"{:30}{:.2f}\".format(\"Observation:\", totals['observation']) )\n",
    "    \n",
    "    \n",
    "    # set up the card\n",
    "    card = dataCard()\n",
    "    card.reset()\n",
    "    card.setPrecision(3)\n",
    "    \n",
    "    # add the uncertainties (just flat ones for now)\n",
    "    card.addUncertainty('lumi', 'lnN')\n",
    "    card.addUncertainty('ttx', 'lnN')\n",
    "    card.addUncertainty('fake', 'lnN')\n",
    "    \n",
    "    # add the single bin\n",
    "    card.addBin('Bin0', [ 'bkg', 'nonprompt' ], 'Bin0')\n",
    "    card.specifyExpectation('Bin0', 'signal', totals['signal'] )\n",
    "    card.specifyExpectation('Bin0', 'bkg', totals['bkg'] )\n",
    "    card.specifyExpectation('Bin0', 'nonprompt', totals['nonprompt'] )\n",
    "    \n",
    "    # set uncertainties\n",
    "    card.specifyUncertainty('ttx', 'Bin0', 'bkg', 1.15 )\n",
    "    card.specifyUncertainty('fake', 'Bin0', 'nonprompt', 1.20 )\n",
    "    card.specifyFlatUncertainty('lumi', 1.03)\n",
    "    \n",
    "    # observation\n",
    "    card.specifyObservation('Bin0', int(round(totals['observation'],0)))\n",
    "    \n",
    "    print (\"Done.\\n\")\n",
    "    \n",
    "    return card.writeToFile(data_card, shapeFile=shape_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_SR = makeCardFromHist('mjj_max', overflow='all')\n",
    "card_SR = makeCardFromHist('mjj_max', nonprompt_scale=0.5, signal_scale=0.12*0.61/0.0478, bkg_scale=1, overflow='all')\n",
    "card_baseline = makeCardFromHist('mjj_max_baseline', overflow='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card = dataCard()\n",
    "results_SR = card.nllScan(fname=card_SR, rmin=0, rmax=5, npoints=101)\n",
    "results_baseline = card.nllScan(fname=card_baseline, rmin=0, rmax=5, npoints=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mplhep\n",
    "plt.style.use(mplhep.style.CMS)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(results_SR['r'][1:], results_SR['deltaNLL'][1:]*2, label='Expected', c='black')#, linewidths=2)\n",
    "plt.plot(results_baseline['r'][1:], results_baseline['deltaNLL'][1:]*2, label='Expected (baseline)', c='red')#, linewidths=2)\n",
    "plt.xlabel(r'$r$')\n",
    "plt.ylabel(r'$-2\\Delta  ln L$')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "plt.style.use(mplhep.cms.style.ROOT)\n",
    "mplhep.cms.cmslabel(data=False, paper=False, year='Run2')\n",
    "\n",
    "#mplhep.mpl_magic() ## doesn't really work.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up in case some combine jobs crashed.\n",
    "card.cleanUp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffeaEnv",
   "language": "python",
   "name": "coffeaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
